]0;IPython: RL-test/gppg_modelGet real data from random policy
num optim already:  0
******************************************************************************************
the 0th rollout begins.
num optim:  0
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: -34.631076
  Number of iterations: 28
  Number of functions evaluations: 33
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: -31.225249
  Number of iterations: 55
  Number of functions evaluations: 63
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: -25.618946
  Number of iterations: 43
  Number of functions evaluations: 50
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: -18.369823
  Number of iterations: 72
  Number of functions evaluations: 82
Finished with GPs' optimization in 4.7 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.65763141] [[0.09316512]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.67511396] [[0.69792267]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.2619984] [[0.40684747]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.20340615] [[0.50081458]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.14069245] [[0.7438582]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [2.64050134] [[1.54091597]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-2.04934714] [[1.21516383]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.26711747] [[0.12880828]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.36678792] [[0.99229832]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.24300307] [[0.28531092]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.98625893] [[0.82775822]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [4.07956669] [[2.18619668]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-1.70934797] [[0.07797142]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.68088175] [[0.10646589]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.66984435] [[0.88588899]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.26888714] [[0.26024261]]
reward for next state distribution:  0.65
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.18906837] [[4.84810903]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [0.79114618] [[4.35273913]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [3.67932717] [[3.88288873]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.03836902] [[0.0582212]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [1.47446839] [[0.18837689]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.05870684] [[0.97844444]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.15660474] [[3.48485421]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [5.42932014] [[3.22408086]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.92366548] [[0.82716238]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.26917232] [[0.14625053]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.01061691] [[1.48359824]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.17629613] [[2.58294402]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-1.55430072] [[0.11582587]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 5th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.70097991] [[0.23404116]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.53635095] [[1.08143642]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.25177199] [[0.21802173]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 6th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.03751122] [[0.08938753]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 7th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.21683036] [[0.01040234]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.90666182] [[0.14780914]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.82764362] [[1.7519346]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 8th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.67473006] [[0.20862801]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 9th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.69575153] [[0.21749534]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.44499798] [[0.83425642]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.18607718] [[0.0609121]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 10th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.21972543] [[0.11121106]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 11th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.25274551] [[0.17043099]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.32929072] [[0.59524853]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.26591883] [[0.17385234]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 12th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [0.97434753] [[0.22023509]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.73949362] [[0.19640531]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.69327019] [[1.6798679]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.21216154] [[4.82629826]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 13th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.08092892] [[0.0796744]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.11703534] [[0.40704688]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.26347249] [[0.26552148]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.79209582] [[2.06390139]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [2.51678998] [[2.15769904]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [0.42174266] [[4.38525916]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 14th init state, total init states: 15
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.91356766] [[0.65145891]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.26687551] [[0.24250771]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 15 times policy/controller optimizations in 24.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 0episodes: 4.23
variance of reward for controller in 0episodes: 0.6171
******************************************************************************************
the 1th rollout begins.
num optim:  15
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 17.518997
  Number of iterations: 16
  Number of functions evaluations: 17
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 17.396942
  Number of iterations: 16
  Number of functions evaluations: 17
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 20.528565
  Number of iterations: 25
  Number of functions evaluations: 29
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 23.000107
  Number of iterations: 45
  Number of functions evaluations: 48
Finished with GPs' optimization in 2.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.39304728] [[0.14006483]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.26339071] [[0.21325761]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.3889316] [[0.09145075]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.13990946] [[0.35145515]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.24346294] [[0.29141]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.89014397] [[0.73479541]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.4819216] [[0.47986401]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.2130062] [[0.36940603]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.63002256] [[1.08342189]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.01629679] [[2.85513052]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.16588753] [[0.34101481]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.68426939] [[0.53291379]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.58802488] [[1.47547425]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.8229646] [[0.09022813]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [0.60632078] [[0.20953631]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 6.6 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 1episodes: 4.13
variance of reward for controller in 1episodes: 0.43309999999999993
******************************************************************************************
the 2th rollout begins.
num optim:  19
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 21.910270
  Number of iterations: 16
  Number of functions evaluations: 17
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 21.949354
  Number of iterations: 16
  Number of functions evaluations: 17
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 25.860018
  Number of iterations: 44
  Number of functions evaluations: 46
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 28.292617
  Number of iterations: 30
  Number of functions evaluations: 34
Finished with GPs' optimization in 2.1 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.4739885] [[0.34205812]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.17132437] [[0.41252691]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.10186284] [[3.22300613]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.2370906] [[3.67757041]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.16362296] [[0.52894528]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.25549406] [[0.3791986]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.08809118] [[0.40276996]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-0.75385748] [[1.20196114]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [1.91422985] [[2.41593845]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.62269553] [[4.3108797]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [0.81238003] [[4.22474366]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.31354047] [[0.19395002]]
reward for next state distribution:  0.65
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.19070656] [[0.42475842]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.25704729] [[3.86800715]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.41686005] [[0.22621241]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.13672383] [[0.46190377]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 7.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 2episodes: 4.25
variance of reward for controller in 2episodes: 0.4075
******************************************************************************************
the 3th rollout begins.
num optim:  24
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 25.470846
  Number of iterations: 17
  Number of functions evaluations: 18
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 25.859182
  Number of iterations: 17
  Number of functions evaluations: 18
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 29.854562
  Number of iterations: 38
  Number of functions evaluations: 40
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 32.221476
  Number of iterations: 30
  Number of functions evaluations: 33
Finished with GPs' optimization in 2.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.19578944] [[0.42328677]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.93371447] [[0.82371207]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.3228942] [[1.20145474]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.19543473] [[0.43109011]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.99787344] [[1.03802995]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.98363613] [[1.02820277]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.2607348] [[0.50841185]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.19513321] [[0.43643275]]
reward for next state distribution:  0.7
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.84345054] [[1.01773519]]
reward for next state distribution:  0.7
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.0870304] [[1.9140926]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.0209397] [[2.29534821]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.84347058] [[2.68197667]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.58530719] [[0.24067836]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [3.87937955] [[0.90713182]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.25356212] [[0.5393221]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 6.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 3episodes: 4.26
variance of reward for controller in 3episodes: 0.35240000000000005
******************************************************************************************
the 4th rollout begins.
num optim:  28
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 30.428653
  Number of iterations: 17
  Number of functions evaluations: 18
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 30.331550
  Number of iterations: 17
  Number of functions evaluations: 18
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 34.765764
  Number of iterations: 32
  Number of functions evaluations: 35
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 37.278824
  Number of iterations: 48
  Number of functions evaluations: 50
Finished with GPs' optimization in 2.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.74334607] [[0.18622023]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [1.54302713] [[0.48784085]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.74729237] [[0.18756932]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [1.58322411] [[0.50063079]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.08102695] [[1.19287683]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.81239307] [[0.36777532]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [3.03470914] [[0.79543575]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.87477346] [[2.16582763]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.08598862] [[1.28225343]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.22441978] [[0.00705846]]
reward for next state distribution:  0.65
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.15123662] [[0.44667344]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.23800778] [[1.51655231]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.3549303] [[1.75856309]]
reward for next state distribution:  0.6
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [4.75120083] [[1.04219087]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-2.2554433] [[0.5695952]]
reward for next state distribution:  0.45
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-0.39082701] [[4.07625086]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.11499811] [[0.4442561]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.27445208] [[1.0502793]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [1.28629148] [[1.2413546]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.28379145] [[3.55827961]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.03864452] [[3.16275262]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-3.02510702] [[2.11478574]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-3.04831999] [[2.09420248]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 10.5 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 4episodes: 4.33
variance of reward for controller in 4episodes: 0.46110000000000007
******************************************************************************************
the 5th rollout begins.
num optim:  33
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 34.126264
  Number of iterations: 17
  Number of functions evaluations: 18
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 34.135880
  Number of iterations: 17
  Number of functions evaluations: 18
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 38.818934
  Number of iterations: 35
  Number of functions evaluations: 40
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 40.835845
  Number of iterations: 29
  Number of functions evaluations: 32
Finished with GPs' optimization in 2.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.05145359] [[0.29706168]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.20383134] [[0.44190174]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.77108364] [[2.90237613]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.22034888] [[2.9807284]]
reward for next state distribution:  1.0
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [5.92244074] [[0.63793299]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.22294067] [[0.18827478]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.1769657] [[0.44113623]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.00752399] [[1.04080065]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [1.11693982] [[2.01961828]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.05217931] [[3.0003547]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17666003] [[0.41661662]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.88452276] [[0.44579355]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.82659267] [[0.64897431]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.27708014] [[0.42699145]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.2396956] [[0.28143338]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.20848185] [[0.42845424]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.76075546] [[0.90061804]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [2.34188583] [[1.30903516]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-1.69252562] [[2.44033293]]
reward for next state distribution:  0.7
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-2.87003041] [[2.33172586]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 8.9 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 5episodes: 4.35
variance of reward for controller in 5episodes: 0.4075
******************************************************************************************
the 6th rollout begins.
num optim:  37
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 36.639202
  Number of iterations: 17
  Number of functions evaluations: 18
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 36.582951
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 41.510826
  Number of iterations: 31
  Number of functions evaluations: 34
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 44.140198
  Number of iterations: 30
  Number of functions evaluations: 36
Finished with GPs' optimization in 2.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.22004102] [[0.40347447]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.0458082] [[0.43244296]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.83595978] [[1.18521998]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.29174722] [[0.37695437]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.23607062] [[0.41450886]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.33331682] [[0.7412338]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [2.14386849] [[0.83535147]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.82177778] [[3.71695385]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-2.94585496] [[2.29010045]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.29707723] [[0.40885894]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.23000452] [[0.40816394]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.82323729] [[3.21466446]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.99122354] [[2.22454876]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-3.04862225] [[2.15038614]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-3.04862543] [[2.15038308]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 7.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 6episodes: 4.18
variance of reward for controller in 6episodes: 0.6875999999999998
******************************************************************************************
the 7th rollout begins.
num optim:  40
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 41.116031
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 41.071503
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 46.463058
  Number of iterations: 33
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 48.524892
  Number of iterations: 43
  Number of functions evaluations: 47
Finished with GPs' optimization in 2.3 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.81050036] [[0.53720352]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.24409367] [[0.39894642]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.09135214] [[1.32804467]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.90406155] [[1.37829205]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.95291075] [[0.24684086]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [2.67316158] [[0.18822414]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.19096719] [[0.39453516]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.783541] [[1.68946347]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.81684986] [[0.5863354]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.24430451] [[0.37715143]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.03363867] [[2.54216686]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.81904694] [[0.60640252]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.2526659] [[0.38290003]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.87374064] [[2.58715867]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.58747051] [[1.68922766]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [4.60071268] [[0.94219811]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [0.6594623] [[0.21669228]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.85923144] [[0.74534758]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.26991895] [[0.3457701]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.91440888] [[2.26236617]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 9.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 7episodes: 4.16
variance of reward for controller in 7episodes: 0.7744
******************************************************************************************
the 8th rollout begins.
num optim:  45
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 45.087702
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 45.154866
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 50.640519
  Number of iterations: 36
  Number of functions evaluations: 41
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 52.667852
  Number of iterations: 36
  Number of functions evaluations: 39
Finished with GPs' optimization in 2.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.2740879] [[0.52224628]]
reward for next state distribution:  0.65
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.22883855] [[0.34325521]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.35572216] [[1.89017921]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.62071625] [[0.61519694]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [7.20487814] [[0.28369193]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.24475649] [[0.3293421]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.25963744] [[0.3199713]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.27804724] [[0.54770947]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.23328761] [[0.32553652]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.14581736] [[0.93510819]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.34311217] [[0.2850403]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-1.92725558] [[0.18211857]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [3.17859443] [[0.04984187]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.2795066] [[0.55836831]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.25540593] [[0.32105768]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.6708339] [[1.3700132]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.10339311] [[0.04544305]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 7.5 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 8episodes: 4.23
variance of reward for controller in 8episodes: 0.7371
******************************************************************************************
the 9th rollout begins.
num optim:  49
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 48.224559
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 47.617150
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 52.843136
  Number of iterations: 34
  Number of functions evaluations: 37
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 55.599972
  Number of iterations: 36
  Number of functions evaluations: 41
Finished with GPs' optimization in 2.5 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.35136319] [[0.56997092]]
reward for next state distribution:  0.7
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.24904166] [[0.3033311]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.9334259] [[0.32599529]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [2.27669861] [[1.13007422]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.35223541] [[0.57773556]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.25113333] [[0.29386215]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.68074302] [[1.69887637]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.06246513] [[0.3157603]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.63554497] [[0.11536497]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.68978213] [[0.0700722]]
reward for next state distribution:  0.3
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-1.05993516] [[0.27807143]]
reward for next state distribution:  0.3
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [-3.04862496] [[2.15390201]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.20981216] [[0.26885956]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.09397913] [[0.13004439]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.54924844] [[0.24643849]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.25756614] [[0.27661623]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.25404588] [[0.28991166]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [1.46563183] [[0.5219112]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 8.2 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 9episodes: 4.08
variance of reward for controller in 9episodes: 0.7335999999999998
******************************************************************************************
the 10th rollout begins.
num optim:  52
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 51.492797
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 51.253891
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 56.724212
  Number of iterations: 43
  Number of functions evaluations: 45
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 59.619332
  Number of iterations: 32
  Number of functions evaluations: 36
Finished with GPs' optimization in 2.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.34534396] [[0.59769512]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.24323191] [[0.25443885]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.00448511] [[0.23103778]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.9969409] [[0.99525629]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.17673782] [[0.22517333]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-1.3425033] [[0.98379577]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.84213376] [[0.3284056]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [2.09210665] [[0.44450607]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.56313367] [[0.40557199]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.96982383] [[1.18810831]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [6.6522665] [[3.18556788]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.12371611] [[0.7565987]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.35618838] [[0.08776575]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [0.42491689] [[1.20023933]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.41578374] [[2.27349667]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.12128332] [[0.7679827]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.34307636] [[0.07505895]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [1.65583064] [[0.21852655]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.31590815] [[0.29953929]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 8.5 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 10episodes: 4.47
variance of reward for controller in 10episodes: 0.8291000000000001
******************************************************************************************
the 11th rollout begins.
num optim:  56
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 56.413187
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 56.125824
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 61.988435
  Number of iterations: 38
  Number of functions evaluations: 41
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 64.473132
  Number of iterations: 38
  Number of functions evaluations: 41
Finished with GPs' optimization in 2.3 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.26474505] [[0.62696395]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.27377249] [[0.16951963]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.32731775] [[0.12763073]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.01911434] [[0.38107895]]
reward for next state distribution:  0.55
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.99010173] [[1.26250724]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [2.17665319] [[1.00257522]]
reward for next state distribution:  0.3
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-2.4961715] [[1.78276686]]
reward for next state distribution:  0.4
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [-1.91195734] [[1.79921842]]
reward for next state distribution:  0.3
Collecting 8th fake data for controller optimization.
mean and variance of action distribution:  [7.23758769] [[2.35991679]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.2645843] [[0.63344717]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.27497079] [[0.13992434]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.02474351] [[0.0161857]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [2.02315054] [[0.61316321]]
reward for next state distribution:  0.65
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.46101414] [[0.01700312]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.56221816] [[0.7499586]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.33831162] [[0.18467717]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.53381375] [[0.56247481]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.35868515] [[0.03497905]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.82331286] [[0.03395613]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.64570248] [[0.59796512]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.27536039] [[0.0252982]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 9.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 11episodes: 4.18
variance of reward for controller in 11episodes: 0.7675999999999997
******************************************************************************************
the 12th rollout begins.
num optim:  61
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 60.002850
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 59.803671
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 65.870351
  Number of iterations: 44
  Number of functions evaluations: 47
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 68.727173
  Number of iterations: 38
  Number of functions evaluations: 41
Finished with GPs' optimization in 2.5 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.20687345] [[0.04513353]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.1025917] [[0.42762553]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.10259277] [[0.10545835]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.13011374] [[0.0727126]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.64991714] [[0.77514212]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [3.88475492] [[0.10814349]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.21537348] [[0.10622184]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.65956628] [[0.82265607]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.27807113] [[0.66998047]]
reward for next state distribution:  0.65
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.31157169] [[0.16316695]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.42015837] [[0.34478471]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.20481325] [[0.11859431]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [0.29382226] [[0.93981524]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-2.28973995] [[1.90151664]]
reward for next state distribution:  0.4
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-2.78063913] [[1.82156129]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 6.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 12episodes: 4.06
variance of reward for controller in 12episodes: 0.8764000000000002
******************************************************************************************
the 13th rollout begins.
num optim:  65
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 63.839155
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 63.339410
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 69.842272
  Number of iterations: 42
  Number of functions evaluations: 45
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 72.600031
  Number of iterations: 43
  Number of functions evaluations: 47
Finished with GPs' optimization in 2.7 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.95314816] [[0.80049624]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.29112373] [[0.1351481]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.35148902] [[0.66908223]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.31118466] [[0.20603596]]
reward for next state distribution:  0.75
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.5885507] [[1.05806103]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [4.07525046] [[0.29767621]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [1.2673379] [[0.11680122]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-1.43282745] [[0.36112399]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.94980816] [[0.81918182]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.27943852] [[0.16546869]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.94847919] [[0.82546724]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.27687594] [[0.1766218]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.34203436] [[0.20254037]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 6.1 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 13episodes: 4.1
variance of reward for controller in 13episodes: 0.7100000000000002
******************************************************************************************
the 14th rollout begins.
num optim:  69
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 68.750721
  Number of iterations: 18
  Number of functions evaluations: 19
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 68.195825
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 74.453990
  Number of iterations: 36
  Number of functions evaluations: 41
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 77.450273
  Number of iterations: 32
  Number of functions evaluations: 35
Finished with GPs' optimization in 2.5 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.11925829] [[0.31573269]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-0.04938094] [[1.0634906]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.58089867] [[0.21302324]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.60516732] [[0.12868923]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.19469213] [[0.25244077]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.44591493] [[0.15438791]]
reward for next state distribution:  0.4
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-0.81091941] [[0.03920387]]
reward for next state distribution:  0.45
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [3.39445203] [[1.16157943]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.12092248] [[0.32629837]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-0.09774604] [[0.90584782]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.25872183] [[0.09933922]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.03597773] [[0.02098387]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.62788685] [[1.08312038]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [3.01025999] [[0.36081039]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.0275794] [[1.12144418]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.38767986] [[0.26086949]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.96349292] [[0.14869426]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.29396337] [[0.99866901]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [2.49819874] [[0.47108185]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [1.80374574] [[0.40116075]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.75244818] [[0.32227182]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 9.2 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 14episodes: 4.05
variance of reward for controller in 14episodes: 0.7074999999999999
******************************************************************************************
the 15th rollout begins.
num optim:  74
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 71.886225
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 71.874725
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 78.109035
  Number of iterations: 35
  Number of functions evaluations: 44
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 81.364696
  Number of iterations: 45
  Number of functions evaluations: 48
Finished with GPs' optimization in 2.7 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.29102685] [[0.46024366]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.2829238] [[0.21246522]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.31327503] [[0.26153787]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.06654491] [[0.99685166]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [3.56972645] [[0.30287836]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.89710769] [[0.45998449]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.19137028] [[0.24804342]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.28377131] [[0.99443669]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.96445681] [[0.0829658]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-4.58315928] [[1.49896388]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [7.33396972] [[0.82671099]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.19136821] [[0.24595894]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.37799672] [[1.07111783]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 5.8 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 15episodes: 4.18
variance of reward for controller in 15episodes: 0.8675999999999996
******************************************************************************************
the 16th rollout begins.
num optim:  78
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 74.704242
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 74.630862
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 80.784634
  Number of iterations: 35
  Number of functions evaluations: 44
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 83.651012
  Number of iterations: 34
  Number of functions evaluations: 39
Finished with GPs' optimization in 2.5 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.31982284] [[0.76771208]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.40636474] [[0.30425433]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.88344654] [[1.20394408]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [1.19874557] [[0.52880539]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [0.63481602] [[0.03659329]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-1.09852942] [[0.58966712]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-1.03732079] [[1.46124614]]
reward for next state distribution:  0.4
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [6.67867498] [[0.49678087]]
reward for next state distribution:  0.55
Collecting 8th fake data for controller optimization.
mean and variance of action distribution:  [-0.21720007] [[0.66799274]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.31994518] [[0.77531688]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.41331603] [[0.29701842]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.60802344] [[1.52299772]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.20152567] [[0.35183545]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.56346598] [[0.15386828]]
reward for next state distribution:  0.5
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.60524439] [[0.13523198]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-2.93635848] [[1.93774324]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.31947773] [[0.78142969]]
reward for next state distribution:  0.7
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.4271272] [[0.29822979]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.27181236] [[1.10393392]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 8.5 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 16episodes: 4.18
variance of reward for controller in 16episodes: 1.0075999999999994
******************************************************************************************
the 17th rollout begins.
num optim:  81
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 79.130053
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 78.153555
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 85.680254
  Number of iterations: 33
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 87.116032
  Number of iterations: 36
  Number of functions evaluations: 39
Finished with GPs' optimization in 2.7 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.49098611] [[1.14956057]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [2.42872156] [[0.61061833]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.70957125] [[1.41016359]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [2.1329744] [[0.80333767]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [3.67788149] [[0.14094861]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15652135] [[0.25827441]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.50190948] [[1.28171209]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [0.899593] [[0.41646563]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.29488809] [[1.37033215]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [7.64078586] [[0.282206]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15588564] [[0.25044354]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.80266206] [[1.31573123]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [3.38146832] [[0.42126987]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.52676912] [[0.51892156]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [3.33913086] [[0.8192132]]
reward for next state distribution:  0.5
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-2.19451396] [[0.29222099]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.36211275] [[0.80546376]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 7.5 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 17episodes: 4.13
variance of reward for controller in 17episodes: 0.7331
******************************************************************************************
the 18th rollout begins.
num optim:  85
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 82.093962
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 81.230959
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 88.460372
  Number of iterations: 45
  Number of functions evaluations: 51
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 91.608152
  Number of iterations: 43
  Number of functions evaluations: 45
Finished with GPs' optimization in 2.7 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.25496912] [[0.79161025]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.38174582] [[0.2533975]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.39635722] [[1.10852878]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [8.24005119] [[0.50351402]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.21267893] [[0.17705667]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.87563438] [[1.02580717]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [3.50337755] [[0.52896451]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.98717735] [[1.1173903]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.63331538] [[0.12207608]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.60948663] [[0.13263028]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.59551149] [[0.14492704]]
reward for next state distribution:  0.4
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [8.11302044] [[0.51196551]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.25380868] [[0.79363012]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.39762888] [[0.2293845]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.40598634] [[1.24962607]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.65644369] [[0.26924377]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.76991002] [[0.14072718]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.2862983] [[0.12792606]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.66293027] [[0.25183401]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.43407802] [[1.63365214]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 9.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 18episodes: 4.11
variance of reward for controller in 18episodes: 0.8179000000000002
******************************************************************************************
the 19th rollout begins.
num optim:  89
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 85.481777
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 86.418020
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 93.241077
  Number of iterations: 35
  Number of functions evaluations: 41
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 94.868224
  Number of iterations: 42
  Number of functions evaluations: 46
Finished with GPs' optimization in 2.7 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.24267609] [[0.12163184]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.31801454] [[0.37566599]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.24973046] [[1.24024165]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.0963767] [[0.08663202]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.24258944] [[0.10516019]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.3675664] [[0.37506673]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.11994033] [[0.93632946]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.24272728] [[0.08605408]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.24286672] [[0.06874963]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.29940907] [[0.34269903]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.68011883] [[1.16431953]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 5.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 19episodes: 4.13
variance of reward for controller in 19episodes: 1.1331000000000002
******************************************************************************************
the 20th rollout begins.
num optim:  93
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 89.312487
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 89.768015
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 96.584164
  Number of iterations: 46
  Number of functions evaluations: 49
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 99.395850
  Number of iterations: 45
  Number of functions evaluations: 50
Finished with GPs' optimization in 2.6 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.30385733] [[0.78840662]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.39676268] [[0.12125164]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.94758334] [[1.41771299]]
reward for next state distribution:  0.65
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [3.65585444] [[0.66398179]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-3.7120004] [[1.24095221]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [6.05374395] [[0.11518459]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-4.29719094] [[0.99925843]]
reward for next state distribution:  0.35
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [-2.91312546] [[1.95584092]]
reward for next state distribution:  0.45
Collecting 8th fake data for controller optimization.
mean and variance of action distribution:  [-3.04124702] [[2.07026839]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.3
Collecting 9th fake data for controller optimization.
Cholesky decomposition failed. In this case, we only take diag element of obs variance
mean and variance of action distribution:  [nan] [[nan]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.25049401] [[0.01180941]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.29334086] [[0.14815059]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.49461739] [[1.33673755]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.56272173] [[0.19197305]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.26549144] [[0.05011744]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.55998908] [[0.16778703]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.27173647] [[0.10071154]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 8.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 20episodes: 4.27
variance of reward for controller in 20episodes: 0.8371000000000001
******************************************************************************************
the 21th rollout begins.
num optim:  97
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 92.972863
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 93.368611
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 100.063767
  Number of iterations: 38
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 101.988050
  Number of iterations: 43
  Number of functions evaluations: 47
Finished with GPs' optimization in 2.8 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.26058136] [[0.76410578]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.31252772] [[0.0599465]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.81840101] [[0.94308337]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [8.10356309] [[0.16718304]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.02744665] [[0.03491589]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.5439185] [[0.29154988]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.26003349] [[0.76603262]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.3481828] [[0.08021911]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.08219717] [[0.47780138]]
reward for next state distribution:  0.6
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [8.56538669] [[0.74480905]]
reward for next state distribution:  0.55
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-1.55386447] [[0.00186695]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.86013791] [[0.73875591]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.04879194] [[0.45992243]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.04062356] [[0.09678847]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.30004139] [[0.97778689]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15936601] [[0.13639508]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.73226955] [[0.86554026]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.97859656] [[0.40279209]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-4.80076512] [[1.59884521]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 8.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 21episodes: 4.18
variance of reward for controller in 21episodes: 1.0275999999999998
******************************************************************************************
the 22th rollout begins.
num optim:  101
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 97.584733
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 98.414044
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 104.819427
  Number of iterations: 34
  Number of functions evaluations: 37
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 107.657726
  Number of iterations: 36
  Number of functions evaluations: 40
Finished with GPs' optimization in 2.8 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.885395] [[1.12119526]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.30872494] [[0.19881103]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.16397481] [[0.00377262]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.14929613] [[0.15590968]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.30388451] [[0.87791306]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [3.70731042] [[0.41085188]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.54492443] [[0.02548348]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [1.02233552] [[0.90294148]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.27263232] [[0.80410225]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.35413477] [[0.17600895]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.48620192] [[0.95502086]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.01456576] [[0.26076263]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.34966277] [[0.15736382]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.16876775] [[0.03329359]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 6.4 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 22episodes: 4.3
variance of reward for controller in 22episodes: 0.89
******************************************************************************************
the 23th rollout begins.
num optim:  106
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 100.666086
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 101.365350
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 109.109401
  Number of iterations: 39
  Number of functions evaluations: 45
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 110.270169
  Number of iterations: 40
  Number of functions evaluations: 43
Finished with GPs' optimization in 3.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.13944651] [[0.15696092]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-0.78579445] [[0.92374641]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [0.22547205] [[0.06562201]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.74468155] [[0.41398276]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [0.60897173] [[0.07852564]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-1.23247898] [[1.55164697]]
reward for next state distribution:  0.45
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-0.09330689] [[0.75587013]]
reward for next state distribution:  0.4
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [1.42715728] [[0.63016592]]
reward for next state distribution:  0.4
Collecting 8th fake data for controller optimization.
mean and variance of action distribution:  [-3.04861559] [[2.16755962]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.79340169] [[6.08314092e-05]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.26074812] [[0.27255813]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.13339838] [[0.22210984]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.31399506] [[1.09176654]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.58092527] [[0.3293364]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.95865015] [[0.20491206]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.12817735] [[0.2804996]]
reward for next state distribution:  0.65
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.12198261] [[0.89317403]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [1.79130701] [[0.47399826]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.94709956] [[0.77013889]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [8.05465941] [[0.18666432]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-1.3327113] [[0.05390524]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 9.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 23episodes: 4.0
variance of reward for controller in 23episodes: 1.02
******************************************************************************************
the 24th rollout begins.
num optim:  110
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 106.130264
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 105.308246
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 112.771390
  Number of iterations: 37
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 115.842510
  Number of iterations: 35
  Number of functions evaluations: 40
Finished with GPs' optimization in 2.9 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.98726838] [[0.8198927]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [1.37325791] [[1.54940206]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.98589185] [[0.81700175]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [1.38456974] [[1.67404765]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.87352121] [[1.35295587]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.07404915] [[1.34932154]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.31658919] [[1.17378275]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.34300164] [[0.4486143]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.29886081] [[0.85469849]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [3.21190601] [[0.8662092]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.10939036] [[0.14950915]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [0.15340274] [[0.26618056]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.98345176] [[0.81198951]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [1.53632271] [[1.84817895]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.75203587] [[0.54013437]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [2.06275477] [[1.59977187]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.94750922] [[0.40469784]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.39619961] [[1.37411765]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 8.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 24episodes: 3.82
variance of reward for controller in 24episodes: 1.0475999999999999
******************************************************************************************
the 25th rollout begins.
num optim:  115
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 107.758344
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 108.974577
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 115.472658
  Number of iterations: 36
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 119.509675
  Number of iterations: 31
  Number of functions evaluations: 34
Finished with GPs' optimization in 2.9 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.14423778] [[0.38841888]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.16589854] [[0.65743633]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.94642566] [[0.36198913]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.85720525] [[1.16884547]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.12350377] [[0.30829985]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.37114273] [[1.42462817]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.37948759] [[0.60422046]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.76737737] [[0.50573128]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.07567567] [[1.49832807]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 4.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 25episodes: 3.69
variance of reward for controller in 25episodes: 1.2738999999999998
******************************************************************************************
the 26th rollout begins.
num optim:  118
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 110.058576
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 111.816242
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 119.638495
  Number of iterations: 35
  Number of functions evaluations: 40
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 121.813302
  Number of iterations: 37
  Number of functions evaluations: 38
Finished with GPs' optimization in 2.9 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.16192147] [[0.71063637]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.79505829] [[0.32717894]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.88584417] [[1.21538264]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.04188598] [[2.05685491]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15946902] [[0.73288182]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.89226147] [[0.37606278]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.76447875] [[1.20676621]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.11880257] [[0.30030987]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.94612317] [[0.42795514]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.15027038] [[1.91843633]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.77175416] [[0.5871373]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 5.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 26episodes: 3.91
variance of reward for controller in 26episodes: 1.3419000000000003
******************************************************************************************
the 27th rollout begins.
num optim:  121
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 113.914278
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 113.858201
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 122.138883
  Number of iterations: 34
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 123.977214
  Number of iterations: 45
  Number of functions evaluations: 48
Finished with GPs' optimization in 2.9 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15442363] [[0.77093791]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.706009] [[0.23716493]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.04222267] [[1.1590748]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-4.62335774] [[0.28740251]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.34144647] [[1.5552098]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.36808238] [[0.69171138]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.11884498] [[1.38173109]]
reward for next state distribution:  0.7
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.01028118] [[0.35894033]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.54043149] [[0.18479639]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.59795624] [[0.16584911]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.12344951] [[0.56220055]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 5.2 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 27episodes: 3.7
variance of reward for controller in 27episodes: 1.1700000000000002
******************************************************************************************
the 28th rollout begins.
num optim:  124
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 120.402813
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 118.690830
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 125.930556
  Number of iterations: 37
  Number of functions evaluations: 40
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 129.905426
  Number of iterations: 32
  Number of functions evaluations: 38
Finished with GPs' optimization in 3.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.70845138] [[0.42515084]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.48388242] [[1.70657736]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.22601348] [[0.84027624]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.58472975] [[0.60471203]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.19453592] [[0.86544451]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.18694609] [[0.87566714]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.32400863] [[0.79324667]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.56896982] [[0.6453675]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.19299736] [[0.89182056]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.35143074] [[0.84063212]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.18457971] [[0.89437834]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 5.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 28episodes: 3.99
variance of reward for controller in 28episodes: 1.3899000000000001
******************************************************************************************
the 29th rollout begins.
num optim:  129
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 123.261420
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 123.295738
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 131.174133
  Number of iterations: 46
  Number of functions evaluations: 50
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 132.701906
  Number of iterations: 38
  Number of functions evaluations: 40
Finished with GPs' optimization in 3.1 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.48510788] [[0.3482383]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.31877849] [[1.52823075]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.73746683] [[0.79064157]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.72372252] [[0.05802576]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.60006942] [[0.17330122]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.82284707] [[0.53868674]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18826878] [[0.91599867]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.35889454] [[1.69338869]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.38615563] [[0.78608467]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.71698516] [[0.04083851]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.93645143] [[0.07154646]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.35996061] [[1.70194677]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.36191247] [[0.81777032]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.96661988] [[0.68679115]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.62515202] [[0.27890628]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.63625259] [[0.3258924]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 7.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 29episodes: 3.88
variance of reward for controller in 29episodes: 1.6856
******************************************************************************************
the 30th rollout begins.
num optim:  133
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 127.401955
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 128.231707
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 137.713575
  Number of iterations: 39
  Number of functions evaluations: 43
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 138.423758
  Number of iterations: 38
  Number of functions evaluations: 43
Finished with GPs' optimization in 2.9 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 6
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.02842212] [[0.0786617]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.13586421] [[2.25868078]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.36764609] [[0.8704723]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.892132] [[1.25235249]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 6
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.03151138] [[0.07593724]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.21357689] [[2.29244095]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.83661985] [[0.76567846]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.02379899] [[0.8794398]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [6.48277725] [[0.10551434]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 6
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.34429109] [[1.75123932]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.37254113] [[0.80484214]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.62726947] [[1.49895251]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [4.86763737] [[1.34297158]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-3.22843859] [[0.18473576]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-1.08025143] [[2.19721341]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-1.08143213] [[2.20487838]]
reward for next state distribution:  0.4
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [6.31388346] [[1.55892947]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 6
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.34478704] [[1.75784088]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.31181458] [[0.81102216]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.2571819] [[1.27390372]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.10199205] [[1.34604309]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 6
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.22739838] [[1.85376017]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.90248855] [[0.0919657]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.22903504] [[0.72456951]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.07896749] [[1.75907263]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.64827427] [[1.20580441]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 5th init state, total init states: 6
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.34552898] [[1.76942157]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.40059269] [[0.78916746]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.04619566] [[0.50787393]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.1801113] [[0.33699288]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [2.48090858] [[1.21149537]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.76505675] [[0.00771072]]
reward for next state distribution:  0.4
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-0.67743609] [[0.09484669]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 6 times policy/controller optimizations in 15.1 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 30episodes: 3.94
variance of reward for controller in 30episodes: 1.3364000000000003
******************************************************************************************
the 31th rollout begins.
num optim:  139
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 131.463152
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 131.246184
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 139.759206
  Number of iterations: 39
  Number of functions evaluations: 46
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 141.057092
  Number of iterations: 35
  Number of functions evaluations: 38
Finished with GPs' optimization in 3.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.75321407] [[1.95296215]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.22107543] [[0.93108922]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.6915838] [[0.38470382]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17312152] [[0.93822565]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.4541496] [[0.72627803]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.08563352] [[1.51706695]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17281936] [[0.93893773]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.5602396] [[0.66249995]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.96392022] [[0.04762682]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.38367883] [[1.80733424]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.43951496] [[0.77248564]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 5.2 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 31episodes: 3.82
variance of reward for controller in 31episodes: 1.0876
******************************************************************************************
the 32th rollout begins.
num optim:  143
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 134.487743
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 134.835526
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 142.210004
  Number of iterations: 44
  Number of functions evaluations: 50
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 145.465902
  Number of iterations: 39
  Number of functions evaluations: 42
Finished with GPs' optimization in 3.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17765898] [[0.0969647]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [3.579882] [[2.51332384]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.72476873] [[0.59872435]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.48637074] [[1.820924]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.47339031] [[0.75226146]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.70271546] [[1.2946924]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.48634485] [[1.82396765]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.49628873] [[0.72214281]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.43727342] [[1.33827779]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 4.1 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 32episodes: 3.93
variance of reward for controller in 32episodes: 1.5451
******************************************************************************************
the 33th rollout begins.
num optim:  146
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 138.548094
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 139.500031
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 146.850370
  Number of iterations: 37
  Number of functions evaluations: 41
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 149.415311
  Number of iterations: 36
  Number of functions evaluations: 41
Finished with GPs' optimization in 3.3 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.38715927] [[1.80620839]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.40467041] [[0.71348237]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.68956841] [[0.50242155]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.82793124] [[0.47176013]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [3.68662979] [[2.41535202]]
reward for next state distribution:  0.6
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [3.68662979] [[2.41535202]]
reward for next state distribution:  0.5
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [3.68662979] [[2.41535202]]
reward for next state distribution:  0.55
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [3.68662979] [[2.41535202]]
reward for next state distribution:  0.55
Collecting 8th fake data for controller optimization.
mean and variance of action distribution:  [3.68662979] [[2.41535202]]
reward for next state distribution:  0.35
Collecting 9th fake data for controller optimization.
mean and variance of action distribution:  [3.68662979] [[2.41535202]]
reward for next state distribution:  0.45
Collecting 10th fake data for controller optimization.
mean and variance of action distribution:  [3.68662979] [[2.41535202]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [1.67454573] [[1.36272793]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.11383321] [[0.31197384]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.25264436] [[1.21724399]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.42035152] [[1.53069449]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.60829309] [[2.26249555]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.70498005] [[0.56840366]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.10447028] [[0.80020766]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.02261838] [[1.23723927]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.29620831] [[1.45067629]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.88782087] [[0.57156811]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [1.11139801] [[1.08005191]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-2.19212336] [[0.89366186]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [1.67309599] [[1.36780384]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.20284743] [[0.41038506]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.78407813] [[1.00208937]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.27127887] [[0.69842473]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-1.11081476] [[0.35472193]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 12.5 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 33episodes: 3.62
variance of reward for controller in 33episodes: 1.3156
******************************************************************************************
the 34th rollout begins.
num optim:  151
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 142.534552
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 141.581927
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 149.247821
  Number of iterations: 36
  Number of functions evaluations: 41
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 151.520700
  Number of iterations: 42
  Number of functions evaluations: 48
Finished with GPs' optimization in 3.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.19127752] [[0.38347285]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.1019228] [[0.74167858]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.88594931] [[1.26035113]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.67903441] [[1.33392088]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-5.39152272] [[1.73082766]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.60190616] [[0.15978448]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.10328276] [[0.73900145]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.0193007] [[1.55720382]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [6.63485381] [[1.10859697]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.34774829] [[0.91206978]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-3.48861014] [[0.27279753]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 5.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 34episodes: 3.8
variance of reward for controller in 34episodes: 1.42
******************************************************************************************
the 35th rollout begins.
num optim:  154
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 145.054139
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 145.546221
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 151.551455
  Number of iterations: 38
  Number of functions evaluations: 41
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 153.775837
  Number of iterations: 45
  Number of functions evaluations: 47
Finished with GPs' optimization in 3.3 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.43784261] [[1.83603854]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.49356235] [[0.63126255]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.84530257] [[0.53775259]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.72584098] [[1.17044194]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-4.74700954] [[0.43773339]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [5.38707912] [[0.02438436]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.20554063] [[0.22230847]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.31212338] [[2.27049797]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.30400853] [[0.82458851]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.12130985] [[0.68883971]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.43735601] [[1.83761611]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.50711101] [[0.58156698]]
reward for next state distribution:  0.65
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [0.08875613] [[1.28173999]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.31476902] [[1.07071494]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.60168409] [[0.15715133]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.18238277] [[0.29807013]]
reward for next state distribution:  0.3
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-0.60196862] [[0.15705092]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 7.9 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 35episodes: 3.71
variance of reward for controller in 35episodes: 1.2858999999999998
******************************************************************************************
the 36th rollout begins.
num optim:  157
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 150.226368
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 149.270619
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 155.577559
  Number of iterations: 39
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 159.990582
  Number of iterations: 40
  Number of functions evaluations: 45
Finished with GPs' optimization in 3.3 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.72766357] [[0.58602037]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [3.80324524] [[1.84457148]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.10717031] [[0.69978886]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.82957197] [[1.53195629]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.02936571] [[0.40700479]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.59338756] [[0.17100832]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.60206828] [[0.1558807]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.72976375] [[0.59001849]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.2917617] [[1.89866906]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.85286743] [[0.4150121]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.51290333] [[0.11257172]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.73071847] [[0.5917985]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [3.6749049] [[1.89968538]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.349285] [[1.82637615]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.7388917] [[0.38950067]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 7.1 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 36episodes: 3.8
variance of reward for controller in 36episodes: 1.32
******************************************************************************************
the 37th rollout begins.
num optim:  162
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 153.182223
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 154.823995
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 160.410929
  Number of iterations: 34
  Number of functions evaluations: 40
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 163.431883
  Number of iterations: 36
  Number of functions evaluations: 41
Finished with GPs' optimization in 3.6 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.11405342] [[0.54081151]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-0.21788857] [[2.02417381]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.93740328] [[0.46207747]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.66410806] [[1.82230772]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.16309796] [[0.91977196]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-4.56021989] [[1.28603484]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.11550514] [[0.53801851]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [0.28729089] [[2.03188557]]
reward for next state distribution:  0.7
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.954518] [[0.6416937]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [2.03353432] [[1.03954911]]
reward for next state distribution:  0.55
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [0.50282698] [[0.06142155]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [1.85239355] [[0.63228668]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-2.91141594] [[0.06217601]]
reward for next state distribution:  0.4
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [-2.51462702] [[2.04712755]]
reward for next state distribution:  0.45
Collecting 8th fake data for controller optimization.
mean and variance of action distribution:  [7.53687909] [[2.24653985]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.11755754] [[0.53320903]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [0.3707904] [[2.02922116]]
reward for next state distribution:  0.7
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.20349269] [[0.72136284]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.83334013] [[0.78958745]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.14316966] [[0.92307434]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.11982186] [[0.52799699]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [0.07042192] [[2.04134369]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.45428304] [[0.23692468]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 10.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 37episodes: 3.94
variance of reward for controller in 37episodes: 1.5364000000000002
******************************************************************************************
the 38th rollout begins.
num optim:  166
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 156.612318
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 155.790913
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 164.562906
  Number of iterations: 37
  Number of functions evaluations: 43
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 167.786704
  Number of iterations: 36
  Number of functions evaluations: 40
Finished with GPs' optimization in 3.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.53766724] [[1.48357496]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.39169034] [[1.8333279]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.59931953] [[0.41654323]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.46850497] [[0.58739496]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.60409006] [[0.15380876]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.60225769] [[0.1462528]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.60206316] [[0.14543609]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.53710516] [[1.48274789]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18981137] [[0.93486831]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.38996927] [[1.83392179]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.58057071] [[0.41779501]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.3387091] [[0.69672412]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.98681776] [[1.76674545]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 6.1 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 38episodes: 3.69
variance of reward for controller in 38episodes: 1.3739000000000001
******************************************************************************************
the 39th rollout begins.
num optim:  170
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 160.504626
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 159.180411
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 167.481167
  Number of iterations: 43
  Number of functions evaluations: 47
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 169.557924
  Number of iterations: 44
  Number of functions evaluations: 46
Finished with GPs' optimization in 3.5 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.39576074] [[1.82574148]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.60635682] [[0.40734093]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.42686369] [[0.4952641]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [2.10980088] [[1.0542299]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.68667269] [[0.05418782]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [5.93066056] [[0.52806455]]
reward for next state distribution:  0.5
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-0.31790578] [[1.75120375]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.02022855] [[0.60244276]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.02243183] [[0.60712629]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [3.89580423] [[1.8705083]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-5.11927524] [[1.42028373]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.12595569] [[0.79915308]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.65918208] [[0.86158976]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [3.98546624] [[1.39803735]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-4.68842432] [[0.53922503]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.60274398] [[0.13418845]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.6059496] [[0.13092161]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 7.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 39episodes: 3.99
variance of reward for controller in 39episodes: 1.2699
******************************************************************************************
the 40th rollout begins.
num optim:  174
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 163.636579
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 165.238132
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 172.245108
  Number of iterations: 33
  Number of functions evaluations: 36
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 176.029427
  Number of iterations: 31
  Number of functions evaluations: 36
Finished with GPs' optimization in 3.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.86951132] [[1.12087675]]
reward for next state distribution:  0.65
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18787209] [[0.92671048]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.1648424] [[0.90673142]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.86926374] [[1.12009954]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.32697184] [[1.81187866]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.73545178] [[0.25331883]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.28431009] [[0.98299383]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.10427252] [[0.8496606]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.28902543] [[0.64577954]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 4.1 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 40episodes: 3.75
variance of reward for controller in 40episodes: 1.0875
******************************************************************************************
the 41th rollout begins.
num optim:  178
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 168.406730
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 168.405156
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 174.471663
  Number of iterations: 37
  Number of functions evaluations: 45
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 178.667840
  Number of iterations: 47
  Number of functions evaluations: 50
Finished with GPs' optimization in 3.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.47625471] [[1.28982809]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18978053] [[0.91822056]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.47614845] [[1.28961624]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18597424] [[0.91977709]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.14218866] [[0.1770377]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.87802867] [[1.55726517]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-4.87552133] [[0.41649565]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.72354487] [[1.26060152]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [0.52188222] [[2.20223266]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.1422449] [[0.17745721]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.99216431] [[1.63844927]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.26111446] [[0.86389769]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.49502032] [[0.37478183]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [5.98257364] [[1.29000489]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-2.17720509] [[0.90654701]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 6.9 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 41episodes: 3.63
variance of reward for controller in 41episodes: 1.2131000000000003
******************************************************************************************
the 42th rollout begins.
num optim:  182
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 172.537750
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 170.127942
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 178.051850
  Number of iterations: 39
  Number of functions evaluations: 44
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 182.184731
  Number of iterations: 43
  Number of functions evaluations: 48
Finished with GPs' optimization in 3.6 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.22519064] [[1.04855085]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.9208518] [[1.57690492]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.32165203] [[0.26076638]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.25696868] [[0.41942]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.47940073] [[0.13492031]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [2.90072604] [[1.28416026]]
reward for next state distribution:  0.5
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-3.49416415] [[0.29000367]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [1.63842988] [[1.74853483]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.13616954] [[0.83973121]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.70515061] [[0.83164909]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.55528461] [[0.14043105]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.47971758] [[0.16440478]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.60012506] [[0.1237249]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.60363297] [[0.12163074]]
reward for next state distribution:  0.3
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-0.60284703] [[0.12243156]]
reward for next state distribution:  0.35
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [7.89894407] [[0.37374252]]
reward for next state distribution:  1.0
Collecting 8th fake data for controller optimization.
mean and variance of action distribution:  [3.68492061] [[2.37665795]]
reward for next state distribution:  1.0
Collecting 9th fake data for controller optimization.
mean and variance of action distribution:  [7.13258445] [[2.02576152]]
reward for next state distribution:  0.85
Collecting 10th fake data for controller optimization.
mean and variance of action distribution:  [7.13258445] [[2.02576152]]
reward for next state distribution:  0.6
Collecting 11th fake data for controller optimization.
mean and variance of action distribution:  [7.13258445] [[2.02576152]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.13595467] [[0.83679733]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 9.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 42episodes: 3.69
variance of reward for controller in 42episodes: 0.9539000000000002
******************************************************************************************
the 43th rollout begins.
num optim:  186
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 175.477889
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 176.001363
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 180.616675
  Number of iterations: 48
  Number of functions evaluations: 51
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 184.934473
  Number of iterations: 45
  Number of functions evaluations: 50
Finished with GPs' optimization in 4.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15360989] [[0.88196238]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.26544178] [[0.35954361]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.21425144] [[1.05948049]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.27869017] [[0.32582766]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [0.57015163] [[1.97171585]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.66660449] [[0.54090614]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.97568791] [[0.33917697]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [0.52186523] [[2.21368113]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15355106] [[0.87684024]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 4.4 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 43episodes: 3.87
variance of reward for controller in 43episodes: 1.1131
******************************************************************************************
the 44th rollout begins.
num optim:  189
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 178.593473
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 181.143582
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 187.421600
  Number of iterations: 35
  Number of functions evaluations: 38
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 191.935540
  Number of iterations: 36
  Number of functions evaluations: 39
Finished with GPs' optimization in 3.6 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17822997] [[0.00081138]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.72110433] [[0.66170292]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18560973] [[0.90168671]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-5.48624942] [[1.87364624]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17709625] [[0.00302492]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.24577363] [[2.01639482]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.94847757] [[0.39720919]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.44164249] [[0.40252466]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.36007191] [[1.84345746]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.92056391] [[0.15949411]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.01408907] [[0.53926606]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.15902744] [[1.12955443]]
reward for next state distribution:  0.55
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-5.21263138] [[1.16721531]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.72127963] [[0.68570186]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18484369] [[0.93060501]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 5th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.16866789] [[0.04045026]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 6th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.72150156] [[0.70655981]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 7 times policy/controller optimizations in 8.1 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 44episodes: 3.71
variance of reward for controller in 44episodes: 1.3859000000000001
******************************************************************************************
the 45th rollout begins.
num optim:  196
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 181.237362
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 182.527137
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 190.649671
  Number of iterations: 48
  Number of functions evaluations: 50
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 193.284371
  Number of iterations: 36
  Number of functions evaluations: 37
Finished with GPs' optimization in 3.9 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.35244711] [[1.78980757]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.54220793] [[0.58257586]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [6.22707324] [[0.2651292]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15222538] [[0.94764093]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.40682858] [[0.2911872]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [7.09172096] [[1.8907744]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.94693316] [[0.78458266]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 2 times policy/controller optimizations in 3.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 45episodes: 3.74
variance of reward for controller in 45episodes: 1.4924000000000002
******************************************************************************************
the 46th rollout begins.
num optim:  198
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 184.794250
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 186.735034
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 192.783662
  Number of iterations: 36
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 196.786772
  Number of iterations: 50
  Number of functions evaluations: 54
Finished with GPs' optimization in 3.6 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.13611505] [[0.93423556]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.46972249] [[0.55781702]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [3.24318513] [[1.3475023]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [4.65317401] [[0.45411782]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.51481449] [[0.88750774]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18327781] [[0.99330275]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.13635534] [[0.95164991]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.51490958] [[0.89613245]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18370242] [[1.00554535]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 4.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 46episodes: 3.75
variance of reward for controller in 46episodes: 1.3275
******************************************************************************************
the 47th rollout begins.
num optim:  202
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 188.620210
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 190.422898
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 197.215274
  Number of iterations: 33
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 200.606444
  Number of iterations: 39
  Number of functions evaluations: 41
Finished with GPs' optimization in 3.8 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.40866065] [[1.77132982]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.39824247] [[0.7570859]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [3.28600568] [[0.11934561]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.89920091] [[0.29844109]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.04120498] [[2.20041775]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.17919801] [[0.96893714]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [5.98360904] [[1.87052656]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.40880802] [[1.76394111]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.51608012] [[0.74378624]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.09329422] [[0.12610678]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.54032614] [[0.72977571]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.10134036] [[1.11566526]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-1.60577747] [[0.67019201]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 6.2 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 47episodes: 3.86
variance of reward for controller in 47episodes: 1.2004
******************************************************************************************
the 48th rollout begins.
num optim:  206
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 191.402810
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 193.517435
  Number of iterations: 19
  Number of functions evaluations: 20
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 201.043189
  Number of iterations: 42
  Number of functions evaluations: 44
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 205.028664
  Number of iterations: 44
  Number of functions evaluations: 47
Finished with GPs' optimization in 4.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.8404544] [[2.32864982]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.2892783] [[0.97770653]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.14959716] [[1.00318715]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.82377978] [[0.02529302]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.02675114] [[1.40566818]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.84048283] [[2.32818702]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.29459529] [[0.97251915]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-5.08591774] [[1.68919043]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.14984209] [[1.00765948]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 4.2 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 48episodes: 3.63
variance of reward for controller in 48episodes: 1.3531000000000004
******************************************************************************************
the 49th rollout begins.
num optim:  210
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 196.224967
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 197.168035
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 204.491930
  Number of iterations: 46
  Number of functions evaluations: 48
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 205.935752
  Number of iterations: 36
  Number of functions evaluations: 40
Finished with GPs' optimization in 4.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.14731511] [[1.00728649]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.94243645] [[0.06687827]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [0.40342725] [[0.52495574]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [2.90611701] [[1.33781057]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-5.09071878] [[0.61132263]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [1.26916014] [[0.81341197]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-1.48559506] [[0.72628461]]
reward for next state distribution:  0.35
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [7.58131092] [[2.02924136]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.28896125] [[1.71288612]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.46929028] [[0.78287564]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.64672679] [[1.11964769]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.58468092] [[0.1841176]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [0.13828128] [[0.43009954]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [0.36069026] [[0.50514186]]
reward for next state distribution:  0.4
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-0.66293562] [[0.11584502]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.36867304] [[2.34629246]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.31462878] [[0.96207918]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-4.28162525] [[0.62032795]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.98351297] [[0.25230808]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.14759353] [[1.01398192]]
reward for next state distribution:  0.65
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.25769401] [[0.18329419]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [7.00200603] [[1.79997149]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.20659613] [[1.03912467]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 10.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 49episodes: 3.58
variance of reward for controller in 49episodes: 1.1635999999999997
******************************************************************************************
the 50th rollout begins.
num optim:  214
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 200.137293
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 200.626706
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 207.512691
  Number of iterations: 49
  Number of functions evaluations: 54
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 211.847829
  Number of iterations: 41
  Number of functions evaluations: 45
Finished with GPs' optimization in 4.1 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15739553] [[1.0246847]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.28005532] [[0.00920279]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.63399698] [[1.24976117]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15743217] [[1.02934685]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.49407255] [[0.19267671]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.61613776] [[0.22694906]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.60486242] [[0.18384644]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.60377515] [[0.18102635]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.31443665] [[1.73660907]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.40897831] [[0.86840871]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.04858573] [[0.41951935]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 5.1 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 50episodes: 3.78
variance of reward for controller in 50episodes: 1.3515999999999997
******************************************************************************************
the 51th rollout begins.
num optim:  217
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 202.038096
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 200.244292
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 209.568421
  Number of iterations: 35
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 212.135495
  Number of iterations: 34
  Number of functions evaluations: 38
Finished with GPs' optimization in 4.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.40627516] [[1.75917439]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.34290082] [[0.90419965]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.9633182] [[0.85253196]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [3.07832969] [[1.40677012]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.98589603] [[2.23888752]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.60804523] [[0.17995806]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15656534] [[1.03976125]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.91920994] [[0.24983621]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.62364983] [[1.50238292]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.12654863] [[0.66170774]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 2 times policy/controller optimizations in 4.8 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 51episodes: 3.86
variance of reward for controller in 51episodes: 1.4203999999999999
******************************************************************************************
the 52th rollout begins.
num optim:  219
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 204.267615
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 204.341197
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 215.030825
  Number of iterations: 36
  Number of functions evaluations: 40
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 215.905874
  Number of iterations: 34
  Number of functions evaluations: 37
Finished with GPs' optimization in 3.8 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.00620145] [[2.10970355]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.63932208] [[0.64276734]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.740465] [[0.1519993]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.27982154] [[2.19466792]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.26991567] [[1.01820525]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-4.55239416] [[1.39261024]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.10140008] [[0.99643418]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.04638914] [[0.69872704]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.70822941] [[1.27894747]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-5.04728492] [[1.4010694]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.34369346] [[1.75290236]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.3806695] [[0.9298543]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.51520761] [[0.34285762]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.60215056] [[0.19177527]]
reward for next state distribution:  1.0
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [3.38800164] [[2.52550028]]
reward for next state distribution:  0.6
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [3.68488603] [[2.41241286]]
reward for next state distribution:  0.85
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-2.77081767] [[1.68706699]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 7.8 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 52episodes: 3.88
variance of reward for controller in 52episodes: 1.0455999999999999
******************************************************************************************
the 53th rollout begins.
num optim:  223
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 208.627288
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 208.000770
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 218.772748
  Number of iterations: 38
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 219.814937
  Number of iterations: 36
  Number of functions evaluations: 39
Finished with GPs' optimization in 4.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.47649752] [[0.11049993]]
reward for next state distribution:  0.7
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.36145281] [[1.75645814]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.36900505] [[1.01857078]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [4.63688722] [[0.72462962]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.17619864] [[1.06757145]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.946403] [[1.12835032]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18262899] [[1.07430866]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17315093] [[1.06906366]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.35576746] [[0.94338464]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17317025] [[1.07047406]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.36292571] [[0.95327411]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.14149044] [[0.84211802]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 5.5 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 53episodes: 3.96
variance of reward for controller in 53episodes: 1.1984
******************************************************************************************
the 54th rollout begins.
num optim:  227
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 213.992246
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 212.150818
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 221.360019
  Number of iterations: 36
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 223.771348
  Number of iterations: 39
  Number of functions evaluations: 43
Finished with GPs' optimization in 4.1 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.3988644] [[1.74294134]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.30514712] [[0.98677995]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.91347363] [[0.48954475]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.02833924] [[0.91596593]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [1.28147659] [[1.45902412]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.07592895] [[0.98329479]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.81279808] [[0.8269039]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [0.96939869] [[0.72261004]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.69477714] [[1.34736884]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.7486303] [[0.04006691]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [4.32197589] [[2.02739791]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.76443862] [[0.94514808]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.07551722] [[0.98721369]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.69916956] [[0.80955525]]
reward for next state distribution:  0.7
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.11713401] [[0.36144999]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.35707499] [[0.27785628]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.55569016] [[0.20929956]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [7.21738631] [[1.43213789]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 8.2 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 54episodes: 3.76
variance of reward for controller in 54episodes: 1.1224000000000003
******************************************************************************************
the 55th rollout begins.
num optim:  231
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 214.012210
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 214.573943
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 219.262853
  Number of iterations: 35
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 226.549769
  Number of iterations: 45
  Number of functions evaluations: 47
Finished with GPs' optimization in 4.5 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.16083424] [[1.06635051]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.96380864] [[0.39237915]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.16087593] [[1.06766288]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.67983847] [[0.58776474]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.10783857] [[2.06994336]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-4.53532904] [[0.12399972]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 2 times policy/controller optimizations in 2.8 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 55episodes: 3.59
variance of reward for controller in 55episodes: 1.2018999999999997
******************************************************************************************
the 56th rollout begins.
num optim:  233
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 214.763329
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 216.672988
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 225.612714
  Number of iterations: 33
  Number of functions evaluations: 40
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 227.808559
  Number of iterations: 37
  Number of functions evaluations: 41
Finished with GPs' optimization in 4.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.11613197] [[1.03265145]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.92027292] [[0.31744271]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.11612951] [[1.033724]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.06645894] [[0.91179235]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.73073517] [[1.22654]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [0.22255641] [[0.4940736]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [3.44457788] [[1.45995518]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-3.98902304] [[1.17354328]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
Cholesky decomposition failed. In this case, we only take diag element of obs variance
mean and variance of action distribution:  [-3.02541304] [[2.28396191]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.45
Collecting 6th fake data for controller optimization.
Cholesky decomposition failed. In this case, we only take diag element of obs variance
mean and variance of action distribution:  [-3.02541304] [[2.28396191]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 5.4 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 56episodes: 3.82
variance of reward for controller in 56episodes: 1.3075999999999999
******************************************************************************************
the 57th rollout begins.
num optim:  236
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 220.286506
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 221.144838
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 228.472917
  Number of iterations: 42
  Number of functions evaluations: 44
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 231.330113
  Number of iterations: 31
  Number of functions evaluations: 35
Finished with GPs' optimization in 4.1 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.38765336] [[1.71789691]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.28446659] [[1.01511533]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [1.80257978] [[0.23574476]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.38748053] [[1.71727647]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.27575207] [[1.02376229]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.06704982] [[0.59136799]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.26176043] [[1.37027939]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.60975619] [[1.43322673]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18571701] [[1.0886054]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.60972965] [[1.43321417]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18750928] [[1.0890244]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 5.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 57episodes: 3.82
variance of reward for controller in 57episodes: 1.1476
******************************************************************************************
the 58th rollout begins.
num optim:  240
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 223.380134
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 223.693586
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 232.518152
  Number of iterations: 39
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 236.805365
  Number of iterations: 44
  Number of functions evaluations: 48
Finished with GPs' optimization in 4.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [0.64262376] [[1.11285474]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.12937872] [[0.52872067]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.22086823] [[0.33096928]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.43815609] [[0.25184031]]
reward for next state distribution:  0.55
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.59800753] [[0.19570019]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.37593754] [[1.71051473]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.30974049] [[1.00275256]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.04141904] [[1.11580987]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.04307535] [[0.85630903]]
reward for next state distribution:  0.55
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.18863228] [[1.07809338]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.89879111] [[2.03226528]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.53275199] [[0.8368719]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.60079183] [[0.62809007]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.14919739] [[1.06410451]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.79802398] [[0.35589251]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [6.70194743] [[2.10989811]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.18293351] [[1.09025992]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 8.2 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 58episodes: 3.9
variance of reward for controller in 58episodes: 0.9699999999999999
******************************************************************************************
the 59th rollout begins.
num optim:  244
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 229.268623
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 231.668496
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 233.152207
  Number of iterations: 35
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 240.739941
  Number of iterations: 32
  Number of functions evaluations: 39
Finished with GPs' optimization in 4.5 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.50298489] [[0.55646583]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [0.14693431] [[1.57216888]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.35050258] [[0.97373005]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.54821746] [[0.21326907]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.60347979] [[0.19353582]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.60444352] [[0.19257277]]
reward for next state distribution:  0.3
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [7.69689034] [[0.67819951]]
reward for next state distribution:  1.0
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [0.52184926] [[2.12262247]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.50290375] [[0.55590143]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.50283027] [[0.55539044]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [0.12995187] [[1.59073594]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.36643222] [[0.50335456]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.93652693] [[1.88837906]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.74777053] [[0.52509051]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.19342336] [[2.20346358]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.25746807] [[1.06075916]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.27249322] [[0.71503117]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.27220746] [[1.23914089]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.34199577] [[0.31077581]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.56197472] [[0.18954822]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.59880421] [[0.18265473]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.63942387] [[0.15473664]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 10.6 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 59episodes: 3.55
variance of reward for controller in 59episodes: 1.1275
******************************************************************************************
the 60th rollout begins.
num optim:  249
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 231.665410
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 233.189968
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 238.766172
  Number of iterations: 38
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 243.962611
  Number of iterations: 43
  Number of functions evaluations: 47
Finished with GPs' optimization in 4.3 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.35512266] [[1.71804167]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.29090581] [[1.0220556]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.86114427] [[0.46814558]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.3549067] [[1.71779038]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.32564282] [[1.00029193]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.6716019] [[0.76280146]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.60211229] [[0.19172811]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.54723908] [[0.21049816]]
reward for next state distribution:  0.55
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [7.91880072] [[0.67812372]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-2.21878393] [[1.15064476]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 2 times policy/controller optimizations in 4.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 60episodes: 3.89
variance of reward for controller in 60episodes: 1.4379
******************************************************************************************
the 61th rollout begins.
num optim:  251
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 232.782499
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 232.863311
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 240.424363
  Number of iterations: 48
  Number of functions evaluations: 56
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 244.041795
  Number of iterations: 31
  Number of functions evaluations: 33
Finished with GPs' optimization in 4.8 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.25501454] [[0.96308117]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18322831] [[1.08910839]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.11334183] [[1.0227416]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.9638816] [[0.3463867]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [6.80786272] [[2.48932418]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.41199416] [[1.73374553]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.2846944] [[1.03403111]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.41178367] [[1.73362691]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.28316369] [[0.99246152]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.46227465] [[0.38518891]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.70359571] [[0.48167112]]
reward for next state distribution:  0.55
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [8.0151053] [[0.19085903]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17658779] [[1.08397502]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.42091822] [[1.01949492]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 7.1 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 61episodes: 3.84
variance of reward for controller in 61episodes: 1.2144000000000004
******************************************************************************************
the 62th rollout begins.
num optim:  255
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 235.884535
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 237.845959
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 245.563324
  Number of iterations: 41
  Number of functions evaluations: 48
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 250.802067
  Number of iterations: 37
  Number of functions evaluations: 42
Finished with GPs' optimization in 4.8 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.07697115] [[1.00143191]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.19174696] [[0.72176422]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [0.17457166] [[0.44909106]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [4.1745958] [[1.47018455]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [5.113286] [[0.27730447]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.72171074] [[2.22993141]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.21881976] [[1.07602927]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.49726002] [[0.30277941]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.62441429] [[0.19317747]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.72147069] [[2.22960169]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.7212538] [[2.22930205]]
reward for next state distribution:  0.7
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.22611924] [[1.07028665]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 5.4 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 62episodes: 3.84
variance of reward for controller in 62episodes: 1.1544000000000003
******************************************************************************************
the 63th rollout begins.
num optim:  259
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 241.596782
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 245.184336
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 251.693568
  Number of iterations: 41
  Number of functions evaluations: 50
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 254.446202
  Number of iterations: 37
  Number of functions evaluations: 40
Finished with GPs' optimization in 4.7 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.34462639] [[1.6886383]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.32758968] [[1.00705311]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.09400567] [[1.59282345]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.3442264] [[1.68828273]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.289909] [[1.01585769]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.69326966] [[0.42983656]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.53111527] [[0.65307519]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.07223291] [[0.99737488]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.07480341] [[0.73561762]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.78696318] [[0.92140787]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [3.60498653] [[1.15553195]]
reward for next state distribution:  0.6
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.29376387] [[1.10617831]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.08065281] [[0.95063765]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.34475331] [[0.99938616]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [6.95635392] [[2.15366714]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.97783562] [[0.87476061]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 7.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 63episodes: 3.77
variance of reward for controller in 63episodes: 1.6570999999999998
******************************************************************************************
the 64th rollout begins.
num optim:  264
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 248.424621
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 246.192609
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 256.608119
  Number of iterations: 42
  Number of functions evaluations: 48
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 258.765746
  Number of iterations: 35
  Number of functions evaluations: 36
Finished with GPs' optimization in 4.7 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [5.48158657] [[1.6644697]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18548856] [[1.08219684]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.3050988] [[0.32425859]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [8.013125] [[0.19295422]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17689765] [[1.08009016]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.36173206] [[1.68276833]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.35379142] [[0.9940322]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.07946662] [[1.15904531]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.61230631] [[0.17214507]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.62130365] [[0.16288571]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [5.48129133] [[1.66386282]]
reward for next state distribution:  0.65
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.19743839] [[1.0796972]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.29467043] [[0.92160058]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 6.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 64episodes: 3.96
variance of reward for controller in 64episodes: 1.3384
******************************************************************************************
the 65th rollout begins.
num optim:  268
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 249.633234
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 252.712138
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 260.874726
  Number of iterations: 41
  Number of functions evaluations: 47
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 263.788352
  Number of iterations: 35
  Number of functions evaluations: 39
Finished with GPs' optimization in 4.9 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.12369693] [[1.03757125]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.53419748] [[0.36299512]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [0.51803272] [[0.55896989]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.64590597] [[0.13728733]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.60978798] [[0.17325784]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.71512533] [[0.06633964]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.39133783] [[2.02459014]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.39968566] [[0.96604289]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.89420851] [[0.78748231]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [1.17866374] [[0.78795829]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [3.30107358] [[1.45402693]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-3.03518346] [[2.25088359]]
reward for next state distribution:  0.3
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [7.57151797] [[1.96992076]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [5.61336377] [[2.11664167]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.19645673] [[1.07872961]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.49678946] [[0.26290177]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.98498981] [[0.57768565]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.98487034] [[0.57746199]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [0.91246013] [[2.03484977]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.86890632] [[0.89764684]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.04834781] [[1.02814594]]
reward for next state distribution:  0.55
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-1.17087604] [[0.58034472]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [1.53622038] [[0.39943171]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 11.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 65episodes: 3.7
variance of reward for controller in 65episodes: 1.47
******************************************************************************************
the 66th rollout begins.
num optim:  273
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 257.774126
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 256.388150
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 264.098005
  Number of iterations: 38
  Number of functions evaluations: 44
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 266.182838
  Number of iterations: 39
  Number of functions evaluations: 52
Finished with GPs' optimization in 4.9 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.22459684] [[0.32081347]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [2.88603821] [[1.97955043]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.22730023] [[0.43299039]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [1.09868182] [[1.31379457]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.23091869] [[0.25264617]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [6.08201073] [[1.022019]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.34302616] [[1.05225606]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [5.11523495] [[0.27335904]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.94065259] [[0.28148912]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [2.57771158] [[2.28433989]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.94052918] [[0.281338]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [2.70872416] [[2.49104381]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.51206983] [[0.64923255]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [1.09791716] [[1.31311708]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.08877396] [[0.19837685]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 7.5 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 66episodes: 3.93
variance of reward for controller in 66episodes: 1.2451
******************************************************************************************
the 67th rollout begins.
num optim:  278
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 263.976288
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 261.000644
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 269.571777
  Number of iterations: 48
  Number of functions evaluations: 51
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 274.465640
  Number of iterations: 39
  Number of functions evaluations: 51
Finished with GPs' optimization in 5.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.04524197] [[1.83307564]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.69659322] [[0.32893069]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [0.94091561] [[1.23649891]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.06564961] [[0.28849285]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.53009129] [[0.21027588]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.13171406] [[0.42548414]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.24312493] [[0.28655468]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [1.56324918] [[0.89562824]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-3.02372829] [[2.30112949]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.17410933] [[0.89247965]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.92978595] [[0.95795247]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [6.52902769] [[1.10368945]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.30681502] [[1.05411792]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [5.11364455] [[0.26538403]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [0.48180515] [[0.91559801]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.76970114] [[0.39671869]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [7.05354043] [[1.87383569]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.486856] [[0.86565536]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.04379419] [[1.83136218]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.73310554] [[0.35762551]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.70659918] [[0.64587417]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 5th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.04339048] [[1.83080966]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.63310966] [[0.45351533]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.57922283] [[0.109823]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.18239443] [[0.26584848]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [1.4613063] [[0.07618093]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 6th init state, total init states: 7
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [0.93893179] [[1.2344521]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.03760581] [[0.27837554]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [7.67793509] [[0.91752252]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.17767251] [[1.07506679]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 7 times policy/controller optimizations in 14.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 67episodes: 3.75
variance of reward for controller in 67episodes: 1.1675
******************************************************************************************
the 68th rollout begins.
num optim:  285
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 266.304020
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 267.294987
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 274.238899
  Number of iterations: 36
  Number of functions evaluations: 43
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 277.287451
  Number of iterations: 45
  Number of functions evaluations: 56
Finished with GPs' optimization in 4.7 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.30523723] [[1.69333806]]
reward for next state distribution:  0.7
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.30920565] [[1.00851677]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.54855086] [[0.27232418]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.30480047] [[1.69288577]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.41150281] [[0.985269]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.95777818] [[1.33677181]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.56503666] [[0.182556]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.60439289] [[0.16796969]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [6.76437716] [[1.89515034]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [5.06196602] [[1.98393028]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.20856483] [[1.06999769]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 5.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 68episodes: 3.96
variance of reward for controller in 68episodes: 1.4784
******************************************************************************************
the 69th rollout begins.
num optim:  288
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 268.649706
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 268.475875
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 277.507276
  Number of iterations: 35
  Number of functions evaluations: 43
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 280.038291
  Number of iterations: 29
  Number of functions evaluations: 31
Finished with GPs' optimization in 4.8 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.12720604] [[1.03200508]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.8519987] [[0.01193267]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.69808644] [[1.16865462]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.96500807] [[2.31685419]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.38754609] [[1.72973326]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.29812868] [[1.01337618]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.17258005] [[0.9986005]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.36374593] [[1.26771924]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [3.68488495] [[2.40201158]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.08657539] [[0.42315611]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [1.87270723] [[1.89427274]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.96297951] [[0.50752525]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.08054951] [[1.05019748]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-4.16728702] [[1.62698403]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.30839885] [[1.84435524]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.66366749] [[0.64092823]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 7.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 69episodes: 3.85
variance of reward for controller in 69episodes: 1.6275000000000006
******************************************************************************************
the 70th rollout begins.
num optim:  292
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 271.228798
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 272.895652
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 278.790559
  Number of iterations: 36
  Number of functions evaluations: 47
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 283.368960
  Number of iterations: 38
  Number of functions evaluations: 50
Finished with GPs' optimization in 5.1 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.13322157] [[1.0456784]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.18547392] [[0.01057436]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.86877826] [[1.00563301]]
reward for next state distribution:  0.65
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.43262247] [[0.98778082]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.13313736] [[1.05157788]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.40042716] [[0.04209956]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.42215735] [[1.72448554]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.2849821] [[1.04224668]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.45171351] [[0.208046]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 4.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 70episodes: 3.67
variance of reward for controller in 70episodes: 1.3411000000000002
******************************************************************************************
the 71th rollout begins.
num optim:  295
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 274.945283
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 275.048886
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 283.904954
  Number of iterations: 37
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 285.111807
  Number of iterations: 46
  Number of functions evaluations: 60
Finished with GPs' optimization in 5.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17324542] [[1.09200135]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.46710817] [[1.00122273]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.9387076] [[0.33573799]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [5.24831788] [[1.76700152]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.19919383] [[1.10094568]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.3522918] [[0.90980014]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [5.24838126] [[1.76627288]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.19360896] [[1.10505903]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.13215232] [[0.33983273]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [5.24843681] [[1.76559722]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.19927048] [[1.10725461]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.55911033] [[0.32488975]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.21279584] [[0.28612082]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.47223836] [[0.1268566]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-1.26102849] [[0.49310494]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [7.55241886] [[1.60627496]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 7.8 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 71episodes: 3.88
variance of reward for controller in 71episodes: 1.2856
******************************************************************************************
the 72th rollout begins.
num optim:  299
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 281.034137
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 279.001667
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 286.438872
  Number of iterations: 32
  Number of functions evaluations: 37
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 290.794464
  Number of iterations: 43
  Number of functions evaluations: 48
Finished with GPs' optimization in 5.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15022199] [[1.08762017]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.80498882] [[0.58147652]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.70264613] [[0.08150114]]
reward for next state distribution:  0.6
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.52418056] [[0.19578235]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [3.28420145] [[1.48720223]]
reward for next state distribution:  0.5
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [5.10451667] [[0.26450063]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.85258819] [[0.43191264]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [2.13568188] [[2.03614949]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.41545188] [[0.77213966]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.75545489] [[0.85360367]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [6.89641622] [[2.12740791]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [0.52184928] [[2.11919881]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [0.52184928] [[2.11919881]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.84943878] [[0.43826153]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [1.63633059] [[1.74287]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.12348855] [[0.34658571]]
reward for next state distribution:  0.6
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [1.51051986] [[0.5783117]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-3.94453684] [[0.48283916]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [1.9327559] [[1.04957961]]
reward for next state distribution:  0.45
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [6.21600814] [[1.49815047]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.74624796] [[1.52020111]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.29297762] [[1.04857755]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.71114409] [[1.74211452]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 10.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 72episodes: 3.88
variance of reward for controller in 72episodes: 1.0656
******************************************************************************************
the 73th rollout begins.
num optim:  303
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 280.979008
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 283.981733
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 291.673467
  Number of iterations: 47
  Number of functions evaluations: 51
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 293.931064
  Number of iterations: 36
  Number of functions evaluations: 41
Finished with GPs' optimization in 5.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.39092857] [[1.68139617]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.24516338] [[1.09337085]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [6.50853467] [[0.91477688]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.73280267] [[0.77918682]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18180696] [[1.11768691]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.58463727] [[0.90842827]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.73270856] [[0.77944249]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18184769] [[1.1171574]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.63277042] [[0.95982282]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-4.1957277] [[0.23225366]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [2.18056921] [[1.13227795]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.39011208] [[1.67898031]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.24934208] [[1.09123585]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.15865684] [[1.45127437]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 7.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 73episodes: 3.93
variance of reward for controller in 73episodes: 1.3850999999999996
******************************************************************************************
the 74th rollout begins.
num optim:  307
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 287.233449
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 287.412548
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 293.424689
  Number of iterations: 45
  Number of functions evaluations: 48
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 296.466058
  Number of iterations: 32
  Number of functions evaluations: 34
Finished with GPs' optimization in 5.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.14644897] [[1.09145518]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.61329068] [[0.73959476]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.20738883] [[0.23483491]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.73293747] [[0.01995824]]
reward for next state distribution:  0.55
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.7325188] [[0.04544465]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.82359867] [[0.04699242]]
reward for next state distribution:  0.3
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-3.02541496] [[2.30066791]]
reward for next state distribution:  0.3
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [7.53666211] [[2.20434311]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.91590193] [[0.68569404]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [0.30517828] [[1.20419893]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.25741874] [[0.88748908]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.73058386] [[0.39842989]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [6.1174416] [[1.37386374]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-4.08809847] [[2.66358218]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.34444346] [[1.64690602]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.34301648] [[1.64654499]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.26070117] [[1.08792792]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.84594466] [[0.0425779]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [1.28725908] [[1.3597912]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 9.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 74episodes: 3.75
variance of reward for controller in 74episodes: 1.3475
******************************************************************************************
the 75th rollout begins.
num optim:  311
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 287.651957
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 287.959393
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 297.841200
  Number of iterations: 41
  Number of functions evaluations: 47
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 298.539001
  Number of iterations: 40
  Number of functions evaluations: 43
Finished with GPs' optimization in 5.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.40500156] [[1.65191584]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.26074073] [[1.08768301]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.04589091] [[2.12592811]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.02541573] [[2.31684068]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [7.53666221] [[2.21933103]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17478729] [[1.10249749]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 2 times policy/controller optimizations in 3.4 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 75episodes: 3.85
variance of reward for controller in 75episodes: 1.3075
******************************************************************************************
the 76th rollout begins.
num optim:  313
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 290.805640
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 288.461204
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 299.418242
  Number of iterations: 41
  Number of functions evaluations: 44
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 301.745605
  Number of iterations: 42
  Number of functions evaluations: 50
Finished with GPs' optimization in 5.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17872144] [[1.10439122]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.23107484] [[1.09859847]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.43104189] [[1.70642428]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.25741752] [[1.09073711]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.02472046] [[2.27467075]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.53666175] [[2.22750253]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 2 times policy/controller optimizations in 2.8 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 76episodes: 3.87
variance of reward for controller in 76episodes: 1.0331000000000001
******************************************************************************************
the 77th rollout begins.
num optim:  315
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 296.857617
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 296.451671
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 303.187905
  Number of iterations: 35
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 307.704347
  Number of iterations: 39
  Number of functions evaluations: 42
Finished with GPs' optimization in 5.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 6
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.02582406] [[0.94223229]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.03585441] [[0.41769058]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.37813146] [[0.17292847]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 6
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.14073158] [[2.08794667]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.23775421] [[1.08599438]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.30108884] [[0.79154634]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 6
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.14644909] [[0.15861186]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [3.01807639] [[1.79862045]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.36829989] [[1.02253393]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.0341367] [[2.11193731]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [6.8155964] [[2.02430515]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [0.52184929] [[2.13933805]]
reward for next state distribution:  0.3
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [0.52184929] [[2.13933805]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 6
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.01818523] [[0.98599765]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.01560079] [[0.19351077]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.14004381] [[0.85018184]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.97418012] [[3.97192654]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-3.01391641] [[2.35141276]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 6
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.27591835] [[1.62762908]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.22888842] [[1.07931452]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.21960765] [[0.94602707]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.18299739] [[1.08671532]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 5th init state, total init states: 6
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.14554279] [[0.15755096]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [2.80015674] [[1.7545423]]
reward for next state distribution:  0.65
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.72800674] [[0.99495968]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 6 times policy/controller optimizations in 12.2 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 77episodes: 4.01
variance of reward for controller in 77episodes: 1.5099
******************************************************************************************
the 78th rollout begins.
num optim:  321
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 298.882688
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 301.067529
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 309.181702
  Number of iterations: 37
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 312.610453
  Number of iterations: 39
  Number of functions evaluations: 41
Finished with GPs' optimization in 5.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17926159] [[1.0968621]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.22341523] [[1.09300839]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.45598156] [[0.19853068]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [3.87597134] [[1.4686732]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.76397178] [[1.10666099]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.52895961] [[2.24456599]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.3765912] [[1.67948541]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.23520657] [[1.08257258]]
reward for next state distribution:  0.65
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.29383273] [[0.70143483]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.96884257] [[0.23543515]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-3.02541655] [[2.33840921]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.65
Collecting 5th fake data for controller optimization.
Cholesky decomposition failed. In this case, we only take diag element of obs variance
mean and variance of action distribution:  [4.26951241] [[1.47859672]]
reward for next state distribution:  0.3
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [4.26951241] [[1.47859672]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.91002503] [[1.83995461]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.1929436] [[1.09631385]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.9041886] [[1.44059647]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.53524676] [[2.2355399]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 8.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 78episodes: 3.93
variance of reward for controller in 78episodes: 1.5250999999999997
******************************************************************************************
the 79th rollout begins.
num optim:  325
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 302.935871
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 303.930488
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 309.995573
  Number of iterations: 36
  Number of functions evaluations: 40
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 316.519638
  Number of iterations: 38
  Number of functions evaluations: 41
Finished with GPs' optimization in 5.5 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.31296208] [[1.6791966]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.27562971] [[1.07140748]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.00224216] [[1.62931218]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.02534806] [[2.33929]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.90546347] [[0.63812701]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [0.94987907] [[1.71344841]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.55911102] [[0.26939997]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.90538077] [[0.63798273]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 3.9 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 79episodes: 3.75
variance of reward for controller in 79episodes: 1.5875
******************************************************************************************
the 80th rollout begins.
num optim:  328
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 308.243072
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 308.310934
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 316.821928
  Number of iterations: 36
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 318.130829
  Number of iterations: 32
  Number of functions evaluations: 36
Finished with GPs' optimization in 5.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.19491182] [[1.61143186]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.39082837] [[0.96302034]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [6.64333592] [[2.71224576]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.18379042] [[1.0964464]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.39930482] [[1.67163125]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.23796277] [[1.08453462]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.35066295] [[0.77956959]]
reward for next state distribution:  0.65
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.01420882] [[0.31089515]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [3.44772039] [[1.3279948]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-2.84864718] [[2.59811062]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [6.6958902] [[3.25509092]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.39909587] [[1.67155042]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.21788804] [[1.08522633]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.85920159] [[0.79803487]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-0.01230051] [[0.29465114]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.94274892] [[0.82998054]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.07478412] [[0.37554662]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.78500112] [[0.05244539]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.8592195] [[0.79762291]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [0.24953981] [[0.41444956]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.89641108] [[1.00069887]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [0.71289582] [[0.50986268]]
reward for next state distribution:  0.55
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.66253652] [[0.42628575]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [0.56344608] [[0.3244485]]
reward for next state distribution:  0.4
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [0.69073353] [[0.5657912]]
reward for next state distribution:  0.5
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [-2.94596039] [[2.31597389]]
reward for next state distribution:  0.3
Collecting 8th fake data for controller optimization.
mean and variance of action distribution:  [3.78523728] [[1.64375511]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 12.9 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 80episodes: 3.74
variance of reward for controller in 80episodes: 1.4324
******************************************************************************************
the 81th rollout begins.
num optim:  333
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 312.105411
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 311.173987
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 314.840277
  Number of iterations: 43
  Number of functions evaluations: 49
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 320.632186
  Number of iterations: 46
  Number of functions evaluations: 51
Finished with GPs' optimization in 5.5 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.87040699] [[0.55837597]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [1.11111151] [[1.37972588]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.44121145] [[0.75008717]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.88464329] [[0.15386752]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [7.20226569] [[2.58790797]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.14887701] [[1.06720709]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.87091881] [[0.55658572]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 3.5 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 81episodes: 3.91
variance of reward for controller in 81episodes: 1.1219000000000001
******************************************************************************************
the 82th rollout begins.
num optim:  336
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 315.372473
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 317.132671
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 319.464206
  Number of iterations: 34
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 324.803157
  Number of iterations: 49
  Number of functions evaluations: 55
Finished with GPs' optimization in 5.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [1.66923664] [[1.28363254]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.57801048] [[0.78091948]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.89980584] [[0.21443384]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.91008578] [[1.40386153]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.19099353] [[1.08758233]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.14174906] [[1.05894062]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.79815005] [[0.54950496]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [0.60471657] [[0.4177828]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.38844467] [[1.67877837]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.22726983] [[1.08279291]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-1.02611579] [[0.01896967]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [1.73789798] [[1.21090347]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-1.19353574] [[0.32933389]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.40178571] [[0.58705675]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [1.66831656] [[1.28058347]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.50731274] [[0.78997281]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.82160903] [[0.26927259]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.01382564] [[1.30856914]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 8.7 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 82episodes: 3.66
variance of reward for controller in 82episodes: 1.1844000000000001
******************************************************************************************
the 83th rollout begins.
num optim:  340
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 312.961538
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 314.948521
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 324.649455
  Number of iterations: 40
  Number of functions evaluations: 47
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 328.417740
  Number of iterations: 36
  Number of functions evaluations: 40
Finished with GPs' optimization in 6.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.31139211] [[1.63331902]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.2285049] [[1.07409177]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.16426452] [[1.45620396]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.82939263] [[0.09656004]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-3.02529179] [[2.34150948]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.31120745] [[1.63178292]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.22447308] [[1.08902991]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.19207742] [[1.01874209]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.22743402] [[0.50266753]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-3.02541669] [[2.34058905]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 2 times policy/controller optimizations in 4.8 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 83episodes: 3.94
variance of reward for controller in 83episodes: 1.1364
******************************************************************************************
the 84th rollout begins.
num optim:  342
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 318.687913
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 318.296661
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 328.209514
  Number of iterations: 36
  Number of functions evaluations: 40
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 330.315930
  Number of iterations: 39
  Number of functions evaluations: 41
Finished with GPs' optimization in 5.3 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.0067526] [[0.80606379]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.22980124] [[0.30171956]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [1.22193711] [[0.53462011]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.01762241] [[1.80347299]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.62387226] [[0.07425167]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [3.86570425] [[1.59189276]]
reward for next state distribution:  0.3
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [5.12474989] [[0.22066544]]
reward for next state distribution:  0.4
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [-1.01605267] [[0.33472726]]
reward for next state distribution:  0.4
Collecting 8th fake data for controller optimization.
mean and variance of action distribution:  [0.52184929] [[2.14445382]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.42167997] [[1.69738947]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.23355524] [[1.08091713]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [6.46054251] [[1.26264939]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [3.9893964] [[2.04385634]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.10507517] [[1.04374924]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [0.02785166] [[0.72889374]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.1967736] [[0.17180535]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 7.8 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 84episodes: 3.93
variance of reward for controller in 84episodes: 1.1850999999999998
******************************************************************************************
the 85th rollout begins.
num optim:  346
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 318.887532
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 323.422368
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 330.859353
  Number of iterations: 35
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 333.067936
  Number of iterations: 34
  Number of functions evaluations: 39
Finished with GPs' optimization in 5.7 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.40307746] [[1.64198592]]
reward for next state distribution:  0.65
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.23675128] [[1.07526898]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.14128948] [[0.86364124]]
reward for next state distribution:  0.6
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.84865596] [[2.03538283]]
reward for next state distribution:  0.5
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [3.9893964] [[2.04269659]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.40273499] [[1.64099973]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.23060918] [[1.08872036]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.04922102] [[2.10312692]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.15929173] [[1.08189071]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.52449211] [[1.03576554]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.33522417] [[1.99198715]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.29515913] [[1.0840384]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 6.1 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 85episodes: 3.64
variance of reward for controller in 85episodes: 1.3304000000000005
******************************************************************************************
the 86th rollout begins.
num optim:  349
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 326.080444
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 322.333079
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 332.532711
  Number of iterations: 36
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 338.761905
  Number of iterations: 62
  Number of functions evaluations: 70
Finished with GPs' optimization in 6.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.33675767] [[1.66137349]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.23031533] [[1.09228272]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.02834728] [[2.29552067]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.01751975] [[0.75823929]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.21363669] [[0.2529822]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.22828199] [[0.18840852]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [2.04721236] [[0.87686264]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.94905934] [[2.41797606]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-3.02541404] [[2.33894223]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.33608531] [[1.65973354]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.27224204] [[1.06582809]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.33572512] [[1.65891075]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.24920594] [[1.07985373]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [6.82916126] [[1.65490379]]
Cholesky decomposition failed in reward calculating. In this case, we only take diag element of obs variance
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 7.1 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 86episodes: 3.99
variance of reward for controller in 86episodes: 1.1498999999999997
******************************************************************************************
the 87th rollout begins.
num optim:  353
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 325.780553
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 328.339477
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 335.862107
  Number of iterations: 39
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 342.013494
  Number of iterations: 43
  Number of functions evaluations: 48
Finished with GPs' optimization in 6.1 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.30714226] [[1.6511789]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.2569947] [[1.07445712]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.93952383] [[1.43377167]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.00920684] [[2.32260651]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.30681067] [[1.65045018]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.23182439] [[1.08277739]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.51633169] [[0.9193071]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.10684454] [[1.67843155]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.30620213] [[1.64913593]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.22341491] [[1.09076762]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 4.9 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 87episodes: 3.75
variance of reward for controller in 87episodes: 1.5875
******************************************************************************************
the 88th rollout begins.
num optim:  357
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 332.935260
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 332.431712
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 337.809513
  Number of iterations: 36
  Number of functions evaluations: 40
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 346.234352
  Number of iterations: 37
  Number of functions evaluations: 42
Finished with GPs' optimization in 6.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.88827289] [[0.67007362]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [0.89332183] [[1.40145172]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.03090924] [[0.11095462]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.36923787] [[0.18984059]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [0.04562454] [[0.317052]]
reward for next state distribution:  0.45
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-1.51174378] [[0.84099825]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-3.02503172] [[2.33888512]]
reward for next state distribution:  0.35
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [7.53667759] [[2.2375424]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.88884477] [[0.66857677]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [1.06610379] [[1.41478765]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.88288541] [[0.25277094]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [3.57015693] [[1.15904912]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-2.64228396] [[2.92894053]]
reward for next state distribution:  0.6
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [4.07325593] [[2.31476778]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.88973798] [[0.66628948]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [0.27365789] [[1.10339312]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.83770217] [[1.32708429]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 8.3 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 88episodes: 3.97
variance of reward for controller in 88episodes: 1.4091
******************************************************************************************
the 89th rollout begins.
num optim:  360
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 333.641488
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 331.685933
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 342.624371
  Number of iterations: 35
  Number of functions evaluations: 43
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 345.888806
  Number of iterations: 57
  Number of functions evaluations: 75
Finished with GPs' optimization in 6.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.16729684] [[1.08701911]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.48708803] [[1.01131022]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.32316353] [[1.64399267]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.25200891] [[1.08089725]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.0526168] [[2.19427286]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 2 times policy/controller optimizations in 3.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 89episodes: 3.87
variance of reward for controller in 89episodes: 1.5731000000000004
******************************************************************************************
the 90th rollout begins.
num optim:  362
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 332.963281
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 337.338307
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 347.756015
  Number of iterations: 37
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 348.020853
  Number of iterations: 41
  Number of functions evaluations: 45
Finished with GPs' optimization in 6.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [2.49124465] [[1.66535164]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.32665844] [[1.00389562]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.98213989] [[2.01540655]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [7.69075158] [[1.10937068]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.09966122] [[1.0400063]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.00232849] [[0.24127371]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.78098345] [[0.75003308]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.14958053] [[1.06756882]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.61838465] [[0.66421399]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.33963244] [[1.36562101]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.30349614] [[1.6304215]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.27416033] [[1.07681048]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.18478889] [[0.21842995]]
reward for next state distribution:  0.55
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.59816384] [[0.10728244]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.43469746] [[0.17266855]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-0.60043472] [[0.10071069]]
reward for next state distribution:  0.55
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-3.023828] [[2.34210032]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 8.5 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 90episodes: 3.86
variance of reward for controller in 90episodes: 1.3403999999999996
******************************************************************************************
the 91th rollout begins.
num optim:  366
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 342.908711
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 340.663761
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 350.824579
  Number of iterations: 38
  Number of functions evaluations: 40
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 354.588986
  Number of iterations: 57
  Number of functions evaluations: 62
Finished with GPs' optimization in 6.1 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.17603243] [[1.08856836]]
reward for next state distribution:  0.75
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.26946761] [[1.08114204]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.65448184] [[0.0705153]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.19170648] [[1.65266977]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [5.47946669] [[1.64685879]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.1959202] [[1.09055212]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.34952818] [[1.6436741]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.22457843] [[1.07449278]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.84555504] [[1.31232242]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.79785969] [[2.80517938]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [5.47916344] [[1.64686107]]
reward for next state distribution:  0.65
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18576927] [[1.08795014]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.23812553] [[0.19821211]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 6.8 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 91episodes: 3.99
variance of reward for controller in 91episodes: 1.5899000000000003
******************************************************************************************
the 92th rollout begins.
num optim:  370
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 342.967708
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 342.192148
  Number of iterations: 21
  Number of functions evaluations: 22
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 352.522614
  Number of iterations: 35
  Number of functions evaluations: 37
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 355.441253
  Number of iterations: 28
  Number of functions evaluations: 31
Finished with GPs' optimization in 6.3 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.53114437] [[1.33981032]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18629671] [[1.08744139]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.13044306] [[1.04856372]]
reward for next state distribution:  0.75
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.59470973] [[0.79683928]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.22127352] [[0.14774729]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.24551457] [[0.15312089]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.69965753] [[0.13704881]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.31174308] [[1.62807758]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.22604892] [[1.06750873]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.3783892] [[0.31444231]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [6.87227973] [[2.0213856]]
reward for next state distribution:  1.0
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 8th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 9th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 10th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 11th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 12th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 13th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 14th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 15th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 16th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 17th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 18th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 19th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 20th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 21th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 22th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 23th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Collecting 24th fake data for controller optimization.
mean and variance of action distribution:  [3.68488945] [[2.38295764]]
reward for next state distribution:  1.0
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 16.1 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 92episodes: 3.9
variance of reward for controller in 92episodes: 1.5899999999999996
******************************************************************************************
the 93th rollout begins.
num optim:  373
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 342.137714
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 343.938903
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 351.395503
  Number of iterations: 33
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 359.277182
  Number of iterations: 35
  Number of functions evaluations: 38
Finished with GPs' optimization in 6.3 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.16775033] [[1.07076124]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.38066811] [[0.9981946]]
reward for next state distribution:  0.45
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.71276456] [[1.94690157]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.48174226] [[1.79592984]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 2
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.34778738] [[1.67555666]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.26213295] [[1.05536164]]
reward for next state distribution:  0.65
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.14127066] [[0.10052721]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 2 times policy/controller optimizations in 3.4 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 93episodes: 3.97
variance of reward for controller in 93episodes: 1.6091
******************************************************************************************
the 94th rollout begins.
num optim:  375
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 348.944148
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 348.739758
  Number of iterations: 21
  Number of functions evaluations: 22
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 355.880180
  Number of iterations: 32
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 363.123441
  Number of iterations: 32
  Number of functions evaluations: 35
Finished with GPs' optimization in 6.0 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.88221359] [[2.27743408]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.20165086] [[1.07364676]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.16294017] [[1.0602102]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.43418665] [[0.94925817]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.96045077] [[0.27165656]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [5.08715229] [[1.40408636]]
reward for next state distribution:  0.45
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-3.39152369] [[1.07228944]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [4.88140713] [[2.27827765]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.31216714] [[1.66650309]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.25518707] [[1.04235926]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 5.5 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 94episodes: 3.88
variance of reward for controller in 94episodes: 1.2255999999999998
******************************************************************************************
the 95th rollout begins.
num optim:  379
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 351.994112
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 354.495360
  Number of iterations: 21
  Number of functions evaluations: 22
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 364.423009
  Number of iterations: 40
  Number of functions evaluations: 45
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 366.741537
  Number of iterations: 34
  Number of functions evaluations: 37
Finished with GPs' optimization in 6.1 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.01393732] [[0.94915945]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.40408691] [[0.25949837]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.50223743] [[0.46242647]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.92791554] [[2.03393461]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-1.90450402] [[1.20860847]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-3.02541674] [[2.35234215]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.13788017] [[1.03349578]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.96301414] [[0.20837401]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [6.88848635] [[2.11769894]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [3.98939639] [[2.06085634]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.1379021] [[1.03191785]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.29805792] [[1.65657319]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.30717057] [[1.01048642]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.25496167] [[1.38256818]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.02566658] [[2.35212556]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 4th init state, total init states: 5
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.47845589] [[1.921542]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.33440434] [[1.02179471]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [4.03284104] [[0.05145798]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-1.97598568] [[0.86595368]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 5 times policy/controller optimizations in 9.5 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 95episodes: 3.78
variance of reward for controller in 95episodes: 1.0515999999999999
******************************************************************************************
the 96th rollout begins.
num optim:  384
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 356.654963
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 357.874608
  Number of iterations: 21
  Number of functions evaluations: 22
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 367.536042
  Number of iterations: 34
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 367.651724
  Number of iterations: 65
  Number of functions evaluations: 72
Finished with GPs' optimization in 6.7 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.927329] [[0.85332744]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-0.82298556] [[0.75284637]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.74102601] [[0.11482631]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.84584752] [[0.13002582]]
reward for next state distribution:  0.35
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [3.54123464] [[1.19524454]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-3.01287582] [[2.37259763]]
reward for next state distribution:  0.3
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [-3.02541673] [[2.35335638]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.92751862] [[0.85265242]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.1863467] [[0.71978608]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-0.36384707] [[0.08424251]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.40577388] [[0.13043197]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-1.08310713] [[0.37150366]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-2.01777007] [[1.32476868]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [7.96534922] [[0.44427828]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [6.48407749] [[1.29541504]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.1902894] [[1.0610436]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [5.11194758] [[0.21953312]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.92783784] [[0.84980186]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-0.45651616] [[0.96333756]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 9.4 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 96episodes: 3.71
variance of reward for controller in 96episodes: 1.1659000000000002
******************************************************************************************
the 97th rollout begins.
num optim:  388
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 358.676519
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 357.640351
  Number of iterations: 33
  Number of functions evaluations: 48
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 368.007694
  Number of iterations: 38
  Number of functions evaluations: 41
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 373.156726
  Number of iterations: 75
  Number of functions evaluations: 81
Finished with GPs' optimization in 7.4 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.1441022] [[1.03048393]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-3.25328795] [[0.26651378]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [6.65031177] [[0.87607357]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-2.19969198] [[1.05945699]]
reward for next state distribution:  0.15
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-1.92701198] [[0.62513783]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 3
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.33259947] [[1.66128183]]
reward for next state distribution:  0.6
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.25972685] [[1.02041153]]
reward for next state distribution:  0.55
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.26426067] [[1.03462216]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 3 times policy/controller optimizations in 4.0 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 97episodes: 3.96
variance of reward for controller in 97episodes: 1.3384
******************************************************************************************
the 98th rollout begins.
num optim:  391
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 361.480374
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 362.057669
  Number of iterations: 26
  Number of functions evaluations: 39
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 372.195548
  Number of iterations: 38
  Number of functions evaluations: 42
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 375.830156
  Number of iterations: 50
  Number of functions evaluations: 56
Finished with GPs' optimization in 7.6 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.3079027] [[0.69855353]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.58757355] [[0.8639372]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.16221546] [[1.03936085]]
reward for next state distribution:  0.45
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.6901389] [[0.78824225]]
reward for next state distribution:  0.4
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [7.00098138] [[2.53318919]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [3.98939638] [[2.06778146]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-0.30838326] [[0.70035728]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-1.67070034] [[0.81315095]]
reward for next state distribution:  0.6
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [2.23420794] [[0.61238645]]
reward for next state distribution:  0.35
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.13904008] [[1.83341047]]
reward for next state distribution:  0.3
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-3.02541657] [[2.35562266]]
reward for next state distribution:  0.3
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [1.78561137] [[0.35994848]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.3763251] [[1.67882254]]
reward for next state distribution:  0.4
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.25685749] [[1.02251984]]
reward for next state distribution:  0.65
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.55290827] [[0.20388641]]
reward for next state distribution:  0.4
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.60254001] [[0.11495319]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.66670142] [[0.04859277]]
reward for next state distribution:  0.35
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [-3.02541382] [[2.35587934]]
reward for next state distribution:  0.1
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 8.9 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 98episodes: 3.93
variance of reward for controller in 98episodes: 1.2451
******************************************************************************************
the 99th rollout begins.
num optim:  395
******************************************************************************************
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 364.893038
  Number of iterations: 20
  Number of functions evaluations: 21
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 367.511733
  Number of iterations: 35
  Number of functions evaluations: 47
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'
  Objective function value: 373.884177
  Number of iterations: 38
  Number of functions evaluations: 41
INFO:tensorflow:Optimization terminated with:
  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'
  Objective function value: 376.310309
  Number of iterations: 35
  Number of functions evaluations: 37
Finished with GPs' optimization in 7.2 seconds
-----Learned models------
---Lengthscales---
       GP0       GP1        GP2        GP3
0   20.115  1259.756  10006.150  39533.395
1   41.463  1855.976  13206.038      6.065
2   55.397     1.621  22944.210  21829.190
3    2.566  1142.799  38139.462    105.909
4  284.680     9.468     13.561     13.526
---Variances---
         GP0    GP1    GP2     GP3
0  7.245e-04  0.011  8.482  43.215
---Noises---
         GP0        GP1        GP2        GP3
0  1.000e-03  1.000e-03  1.000e-03  1.000e-03
------------------------------------------------------------
Evaluate the 0th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [-2.18000438] [[1.0520443]]
reward for next state distribution:  0.3
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.22996987] [[1.04545916]]
reward for next state distribution:  0.2
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 1th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.33609581] [[1.68255097]]
reward for next state distribution:  0.5
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.29883682] [[0.9961219]]
reward for next state distribution:  0.5
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.35932515] [[0.88280613]]
reward for next state distribution:  0.3
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.06033836] [[2.20977138]]
reward for next state distribution:  0.05
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 2th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [7.16768148] [[0.95550393]]
reward for next state distribution:  0.35
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.18619932] [[1.05053551]]
reward for next state distribution:  0.3
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-2.56540468] [[0.98143867]]
reward for next state distribution:  0.5
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-3.12084301] [[1.94663736]]
reward for next state distribution:  0.0
Now we begin the optimization for controller.
Controller optimization finished.
------------------------------------------------------------
Evaluate the 3th init state, total init states: 4
------------------------------------------------------------
Collecting 0th fake data for controller optimization.
mean and variance of action distribution:  [3.33565003] [[1.68291804]]
reward for next state distribution:  0.55
Collecting 1th fake data for controller optimization.
mean and variance of action distribution:  [-2.28305176] [[1.00777515]]
reward for next state distribution:  0.35
Collecting 2th fake data for controller optimization.
mean and variance of action distribution:  [-3.65157526] [[0.40917541]]
reward for next state distribution:  0.45
Collecting 3th fake data for controller optimization.
mean and variance of action distribution:  [-0.35455477] [[0.15816501]]
reward for next state distribution:  0.4
Collecting 4th fake data for controller optimization.
mean and variance of action distribution:  [-0.53340649] [[0.13182451]]
reward for next state distribution:  0.4
Collecting 5th fake data for controller optimization.
mean and variance of action distribution:  [2.11785127] [[1.01533551]]
reward for next state distribution:  0.35
Collecting 6th fake data for controller optimization.
mean and variance of action distribution:  [7.25374736] [[1.22780205]]
reward for next state distribution:  0.35
Collecting 7th fake data for controller optimization.
mean and variance of action distribution:  [-2.1551013] [[1.02293135]]
reward for next state distribution:  0.3
Collecting 8th fake data for controller optimization.
mean and variance of action distribution:  [-2.17308753] [[1.04706817]]
reward for next state distribution:  0.25
Now we begin the optimization for controller.
Controller optimization finished.
Finished with 4 times policy/controller optimizations in 9.6 seconds
saving the controller.
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
average reward for controller in 99episodes: 3.89
variance of reward for controller in 99episodes: 1.3779
model saved in file: ./checkpoints/InvertedPendulum-v2/actor.ckpt
Finished !
Episode:  0  step in this episode: 3  reward:  3.0
Episode:  1  step in this episode: 2  reward:  2.0
Episode:  2  step in this episode: 4  reward:  4.0
Episode:  3  step in this episode: 5  reward:  5.0
Episode:  4  step in this episode: 4  reward:  4.0
Episode:  5  step in this episode: 2  reward:  2.0
Episode:  6  step in this episode: 2  reward:  2.0
Episode:  7  step in this episode: 4  reward:  4.0
Episode:  8  step in this episode: 3  reward:  3.0
Episode:  9  step in this episode: 5  reward:  5.0
Episode:  10  step in this episode: 4  reward:  4.0
Episode:  11  step in this episode: 4  reward:  4.0
Episode:  12  step in this episode: 4  reward:  4.0
Episode:  13  step in this episode: 2  reward:  2.0
Episode:  14  step in this episode: 3  reward:  3.0
Episode:  15  step in this episode: 4  reward:  4.0
Episode:  16  step in this episode: 4  reward:  4.0
Episode:  17  step in this episode: 3  reward:  3.0
Episode:  18  step in this episode: 5  reward:  5.0
Episode:  19  step in this episode: 4  reward:  4.0
Episode:  20  step in this episode: 4  reward:  4.0
Episode:  21  step in this episode: 4  reward:  4.0
Episode:  22  step in this episode: 5  reward:  5.0
Episode:  23  step in this episode: 2  reward:  2.0
Episode:  24  step in this episode: 4  reward:  4.0
Episode:  25  step in this episode: 3  reward:  3.0
Episode:  26  step in this episode: 2  reward:  2.0
Episode:  27  step in this episode: 4  reward:  4.0
Episode:  28  step in this episode: 4  reward:  4.0
Episode:  29  step in this episode: 4  reward:  4.0
Episode:  30  step in this episode: 4  reward:  4.0
Episode:  31  step in this episode: 2  reward:  2.0
Episode:  32  step in this episode: 5  reward:  5.0
Episode:  33  step in this episode: 2  reward:  2.0
Episode:  34  step in this episode: 2  reward:  2.0
Episode:  35  step in this episode: 5  reward:  5.0
Episode:  36  step in this episode: 3  reward:  3.0
Episode:  37  step in this episode: 4  reward:  4.0
Episode:  38  step in this episode: 6  reward:  6.0
Episode:  39  step in this episode: 4  reward:  4.0
Episode:  40  step in this episode: 7  reward:  7.0
Episode:  41  step in this episode: 4  reward:  4.0
Episode:  42  step in this episode: 4  reward:  4.0
Episode:  43  step in this episode: 3  reward:  3.0
Episode:  44  step in this episode: 2  reward:  2.0
Episode:  45  step in this episode: 4  reward:  4.0
Episode:  46  step in this episode: 3  reward:  3.0
Episode:  47  step in this episode: 3  reward:  3.0
Episode:  48  step in this episode: 4  reward:  4.0
Episode:  49  step in this episode: 4  reward:  4.0
Episode:  50  step in this episode: 3  reward:  3.0
Episode:  51  step in this episode: 7  reward:  7.0
Episode:  52  step in this episode: 4  reward:  4.0
Episode:  53  step in this episode: 2  reward:  2.0
Episode:  54  step in this episode: 4  reward:  4.0
Episode:  55  step in this episode: 4  reward:  4.0
Episode:  56  step in this episode: 2  reward:  2.0
Episode:  57  step in this episode: 5  reward:  5.0
Episode:  58  step in this episode: 4  reward:  4.0
Episode:  59  step in this episode: 6  reward:  6.0
Episode:  60  step in this episode: 2  reward:  2.0
Episode:  61  step in this episode: 5  reward:  5.0
Episode:  62  step in this episode: 5  reward:  5.0
Episode:  63  step in this episode: 3  reward:  3.0
Episode:  64  step in this episode: 4  reward:  4.0
Episode:  65  step in this episode: 4  reward:  4.0
Episode:  66  step in this episode: 3  reward:  3.0
Episode:  67  step in this episode: 2  reward:  2.0
Episode:  68  step in this episode: 4  reward:  4.0
Episode:  69  step in this episode: 6  reward:  6.0
Episode:  70  step in this episode: 3  reward:  3.0
Episode:  71  step in this episode: 3  reward:  3.0
Episode:  72  step in this episode: 2  reward:  2.0
Episode:  73  step in this episode: 4  reward:  4.0
Episode:  74  step in this episode: 5  reward:  5.0
Episode:  75  step in this episode: 4  reward:  4.0
Episode:  76  step in this episode: 4  reward:  4.0
Episode:  77  step in this episode: 2  reward:  2.0
Episode:  78  step in this episode: 5  reward:  5.0
Episode:  79  step in this episode: 4  reward:  4.0
Episode:  80  step in this episode: 4  reward:  4.0
Episode:  81  step in this episode: 3  reward:  3.0
Episode:  82  step in this episode: 3  reward:  3.0
Episode:  83  step in this episode: 3  reward:  3.0
Episode:  84  step in this episode: 2  reward:  2.0
Episode:  85  step in this episode: 4  reward:  4.0
Episode:  86  step in this episode: 4  reward:  4.0
Episode:  87  step in this episode: 4  reward:  4.0
Episode:  88  step in this episode: 4  reward:  4.0
Episode:  89  step in this episode: 4  reward:  4.0
Episode:  90  step in this episode: 4  reward:  4.0
Episode:  91  step in this episode: 5  reward:  5.0
Episode:  92  step in this episode: 2  reward:  2.0
Episode:  93  step in this episode: 4  reward:  4.0
Episode:  94  step in this episode: 3  reward:  3.0
Episode:  95  step in this episode: 3  reward:  3.0
Episode:  96  step in this episode: 4  reward:  4.0
Episode:  97  step in this episode: 2  reward:  2.0
Episode:  98  step in this episode: 4  reward:  4.0
Episode:  99  step in this episode: 5  reward:  5.0
Episode:  0  step in this episode: 3  reward:  3.0
Episode:  1  step in this episode: 3  reward:  3.0
Episode:  2  step in this episode: 4  reward:  4.0
Episode:  3  step in this episode: 9  reward:  9.0
Episode:  4  step in this episode: 4  reward:  4.0
Episode:  5  step in this episode: 3  reward:  3.0
Episode:  6  step in this episode: 7  reward:  7.0
Episode:  7  step in this episode: 6  reward:  6.0
Episode:  8  step in this episode: 3  reward:  3.0
Episode:  9  step in this episode: 4  reward:  4.0
Episode:  10  step in this episode: 4  reward:  4.0
Episode:  11  step in this episode: 10  reward:  10.0
Episode:  12  step in this episode: 3  reward:  3.0
Episode:  13  step in this episode: 3  reward:  3.0
Episode:  14  step in this episode: 4  reward:  4.0
Episode:  15  step in this episode: 4  reward:  4.0
Episode:  16  step in this episode: 4  reward:  4.0
Episode:  17  step in this episode: 6  reward:  6.0
Episode:  18  step in this episode: 3  reward:  3.0
Episode:  19  step in this episode: 3  reward:  3.0
Episode:  20  step in this episode: 6  reward:  6.0
Episode:  21  step in this episode: 3  reward:  3.0
Episode:  22  step in this episode: 4  reward:  4.0
Episode:  23  step in this episode: 3  reward:  3.0
Episode:  24  step in this episode: 4  reward:  4.0
Episode:  25  step in this episode: 4  reward:  4.0
Episode:  26  step in this episode: 5  reward:  5.0
Episode:  27  step in this episode: 5  reward:  5.0
Episode:  28  step in this episode: 6  reward:  6.0
Episode:  29  step in this episode: 5  reward:  5.0
Episode:  30  step in this episode: 5  reward:  5.0
Episode:  31  step in this episode: 3  reward:  3.0
Episode:  32  step in this episode: 4  reward:  4.0
Episode:  33  step in this episode: 3  reward:  3.0
Episode:  34  step in this episode: 7  reward:  7.0
Episode:  35  step in this episode: 3  reward:  3.0
Episode:  36  step in this episode: 6  reward:  6.0
Episode:  37  step in this episode: 6  reward:  6.0
Episode:  38  step in this episode: 13  reward:  13.0
Episode:  39  step in this episode: 5  reward:  5.0
Episode:  40  step in this episode: 4  reward:  4.0
Episode:  41  step in this episode: 3  reward:  3.0
Episode:  42  step in this episode: 6  reward:  6.0
Episode:  43  step in this episode: 4  reward:  4.0
Episode:  44  step in this episode: 8  reward:  8.0
Episode:  45  step in this episode: 3  reward:  3.0
Episode:  46  step in this episode: 3  reward:  3.0
Episode:  47  step in this episode: 5  reward:  5.0
Episode:  48  step in this episode: 3  reward:  3.0
Episode:  49  step in this episode: 8  reward:  8.0
Episode:  50  step in this episode: 3  reward:  3.0
Episode:  51  step in this episode: 6  reward:  6.0
Episode:  52  step in this episode: 3  reward:  3.0
Episode:  53  step in this episode: 4  reward:  4.0
Episode:  54  step in this episode: 9  reward:  9.0
Episode:  55  step in this episode: 3  reward:  3.0
Episode:  56  step in this episode: 7  reward:  7.0
Episode:  57  step in this episode: 8  reward:  8.0
Episode:  58  step in this episode: 3  reward:  3.0
Episode:  59  step in this episode: 3  reward:  3.0
Episode:  60  step in this episode: 6  reward:  6.0
Episode:  61  step in this episode: 4  reward:  4.0
Episode:  62  step in this episode: 3  reward:  3.0
Episode:  63  step in this episode: 12  reward:  12.0
Episode:  64  step in this episode: 11  reward:  11.0
Episode:  65  step in this episode: 5  reward:  5.0
Episode:  66  step in this episode: 5  reward:  5.0
Episode:  67  step in this episode: 18  reward:  18.0
Episode:  68  step in this episode: 5  reward:  5.0
Episode:  69  step in this episode: 5  reward:  5.0
Episode:  70  step in this episode: 6  reward:  6.0
Episode:  71  step in this episode: 4  reward:  4.0
Episode:  72  step in this episode: 3  reward:  3.0
Episode:  73  step in this episode: 5  reward:  5.0
Episode:  74  step in this episode: 10  reward:  10.0
Episode:  75  step in this episode: 3  reward:  3.0
Episode:  76  step in this episode: 4  reward:  4.0
Episode:  77  step in this episode: 3  reward:  3.0
Episode:  78  step in this episode: 3  reward:  3.0
Episode:  79  step in this episode: 3  reward:  3.0
Episode:  80  step in this episode: 3  reward:  3.0
Episode:  81  step in this episode: 4  reward:  4.0
Episode:  82  step in this episode: 8  reward:  8.0
Episode:  83  step in this episode: 13  reward:  13.0
Episode:  84  step in this episode: 3  reward:  3.0
Episode:  85  step in this episode: 10  reward:  10.0
Episode:  86  step in this episode: 4  reward:  4.0
Episode:  87  step in this episode: 3  reward:  3.0
Episode:  88  step in this episode: 8  reward:  8.0
Episode:  89  step in this episode: 3  reward:  3.0
Episode:  90  step in this episode: 8  reward:  8.0
Episode:  91  step in this episode: 4  reward:  4.0
Episode:  92  step in this episode: 11  reward:  11.0
Episode:  93  step in this episode: 7  reward:  7.0
Episode:  94  step in this episode: 4  reward:  4.0
Episode:  95  step in this episode: 4  reward:  4.0
Episode:  96  step in this episode: 4  reward:  4.0
Episode:  97  step in this episode: 4  reward:  4.0
Episode:  98  step in this episode: 3  reward:  3.0
Episode:  99  step in this episode: 6  reward:  6.0
average reward for first policy:  3.68
variance of reward for first policy:  1.2776000000000003
average reward for second policy:  5.18
variance of reward for second policy:  7.5876
figure store in: ./checkpoints/InvertedPendulum-v2/actor_ep_reward.png
